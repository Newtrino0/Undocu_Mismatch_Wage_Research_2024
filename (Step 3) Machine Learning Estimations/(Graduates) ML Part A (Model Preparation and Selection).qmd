---
title: "ML Undocu imputation (SIPP to ACS)"
format: html
editor: visual
---

## Quarto

Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see <https://quarto.org>.

## Running Code

When you click the **Render** button a document will be generated that includes both content and the output of embedded code. You can embed code like this:

## Read SIPP data (2008 Wave 2)

```{r}
data_path <- "G:/Shared drives/Undocu Research/Data"
setwd(data_path)

library(readr)
sipp08_2 <- read_csv("(Step 1 output) Core_TM SIPP 2008 Wave 2.csv")
View(sipp08_2)
```

## SIPP data preparation and cleaning

```{r}
library(dplyr)
sipp08_2 <- sipp08_2 %>%
  mutate(
    undocu_entry = as.factor(ifelse(timstat=="Other", 1, 0)),
    undocu_likely = as.factor(ifelse(timstat=="Other" & eadjust=="No", 1, 0)),
    education = case_when(
      eeducate == "10th Grade"  | eeducate == "11th Grade" | eeducate == "12th grade, no diploma" | eeducate == "1st, 2nd, 3rd, or 4th grade" | eeducate == "5th Or 6th Grade" | eeducate == "7th Or 8th Grade" | eeducate == "9th Grade" | eeducate == "Less Than 1st Grade"~ "No HS diploma",
      eeducate == "Diploma or certificate from a" | eeducate == "High School Graduate - (diploma" ~ "HS diploma",
      eeducate == "Some college, but no degree" ~ "Some college",
      eeducate =="Associate (2-yr) college degree" ~ "Associate's",
      eeducate == "Bachelor's degree (for example:" ~ "Bachelor's",
      eeducate == "Master's degree (For example: MA," ~ "Master's",
      eeducate == "Doctorate degree (for example:" ~ "PhD",
      TRUE ~ "Unknown" # Default case
    ),
    yrsed = case_when(
      eeducate == "10th Grade"~10,
      eeducate == "11th Grade"~11,
      eeducate == "12th grade, no diploma" | eeducate == "Diploma or certificate from a" | eeducate == "High School Graduate - (diploma" | eeducate == "Some college, but no degree" ~12,
      eeducate == "1st, 2nd, 3rd, or 4th grade"~2.5,
      eeducate == "5th Or 6th Grade"~5.5,
      eeducate == "7th Or 8th Grade"~7.5,
      eeducate == "9th Grade"~9,
      eeducate == "Less Than 1st Grade"~0,
      eeducate =="Associate (2-yr) college degree"~14,
      eeducate == "Bachelor's degree (for example:"~16,
      eeducate == "Master's degree (For example: MA,"~17.5,
      eeducate == "Doctorate degree (for example:"~22,
      eeducate == "Professional School degree (for"~16,
      TRUE ~ NA
    ),
    college = as.factor(ifelse(eeducate=="Bachelor's degree (for example:" | eeducate=="Master's degree (For example: MA," | eeducate=="Doctorate degree (for example:", 1, 0)),
    hs_only = as.factor(ifelse(eeducate=="Some college, but no degree" | eeducate== "Associate (2-yr) college degree" | eeducate=="High School Graduate - (diploma" | eeducate=="Diploma or certificate from a", 1, 0)),
    immig_yr = case_when(
      tmoveus == "1961"~1961,
      tmoveus == "1961-1968"~1966,
      tmoveus == "1969-1973"~1971,
      tmoveus == "1974-1978"~1976,
      tmoveus == "1979-1980"~1980,
      tmoveus == "1981-1983"~1982,
      tmoveus == "1984-1985"~1984,
      tmoveus == "1986-1988"~1987,
      tmoveus == "1989-1990"~1989,
      tmoveus == "1991-1992"~1991,
      tmoveus == "1993-1994"~1993,
      tmoveus == "1995-1996"~1995,
      tmoveus == "1997-1998"~1998,
      tmoveus == "1999"~1999,
      tmoveus == "2000"~2000,
      tmoveus == "2001"~2001,
      tmoveus == "2002-2003"~2002,
      tmoveus == "2004"~2004,
      tmoveus == "2005"~2005,
      tmoveus == "2006"~2006,
      tmoveus == "2007"~2007,
      tmoveus == "2008-2009"~2009,
      TRUE ~ 0 # Default case
    ),
    married = as.factor(ifelse(ems=="Married, spouse absent" | ems=="Married, spouse present", 1, 0)),
    english_difficult = as.factor(ifelse(ehowwell=="Not at all" | ehowwell=="Not well", 1, 0)),
    nonfluent = as.factor(ifelse(ehowwell=="Not at all" | ehowwell=="Not well", 1, 0)),
    english_home = as.factor(ifelse(tlang1=="Not in Universe", 1, 0)),
    spanish_hispanic_latino = as.factor(ifelse(eorigin=="Yes", 1, 0)),
    medicaid = as.factor(ifelse(rcutyp57=="Yes, covered", 1, 0)),
    household_size = ehhnumpp,
    race = case_when(
      erace=="Asian alone" ~ "Asian",
      erace=="Black alone" ~ "Black",
      erace=="White alone" ~ "White",
      erace=="Residual" ~ "Other",
      TRUE ~ "Unknown"
    ),
    fem = as.factor(ifelse(esex=="Female", 1, 0)),
    asian = as.factor(ifelse(erace=="Asian alone", 1, 0)),
    black = as.factor(ifelse(erace=="Black alone", 1, 0)),
    white = as.factor(ifelse(erace=="White alone", 1, 0)),
    other_race = as.factor(ifelse(erace=="Residual", 1, 0)),
    employed = as.factor(ifelse(rmesr=="With a job at least 1 but not all" | rmesr=="With a job entire month, absent" | rmesr=="With a job entire month, worked", 1, 0)),
    years_us = rhcalyr - immig_yr,
    citizen = as.factor(ifelse(ecitizen=="Yes", 1, 0)),
    cit_spouse = as.factor(cit_spouse),
    poverty = as.factor(ifelse(thearn<rhpov, 1, 0)),
    armed_forces = as.factor(ifelse(eafnow=="Yes" | eafever=="Yes", 1, 0)),
    health_ins= as.factor(ifelse(rcutyp57=="Yes, covered" | rcutyp58=="Yes, covered" , 1, 0)),
    medicare = as.factor(ifelse(ecrmth=="Yes, covered", 1, 0)),
    social_security = as.factor(ifelse(rcutyp01=="Yes, covered" | rcutyp03=="Yes, covered", 1, 0)),
    central_latino = as.factor(ifelse(tbrstate=="Central America" & eorigin=="Yes", 1, 0)),
    bpl_usa = as.factor(ifelse(ebornus=="Yes", 1, 0)),
    bpl_asia = as.factor(ifelse(tbrstate == "Eastern Asia"| tbrstate == "South Central Asia"| tbrstate == "South East Asia, West Asia,", 1, 0)),
    top_ten_states = as.factor(ifelse(tfipsst=="California" | tfipsst=="Texas" | tfipsst=="Florida" | tfipsst=="New Jersey" | tfipsst=="Illinois" | tfipsst=="New York" | tfipsst=="North Carolina" | tfipsst=="Georgia" | tfipsst=="Washington" | tfipsst=="Arizona", 1, 0))
  )

sipp08_2$bpl_foreign <- as.factor(ifelse(sipp08_2$bpl_usa==1, 0, 1))
sipp08_2$undocu_likely <- replace(sipp08_2$undocu_likely, sipp08_2$immig_yr <= 1961, 0)
sipp08_2$years_us <- ifelse(sipp08_2$years_us == 2008 | sipp08_2$years_us == 2009 | sipp08_2$years_us == -1 , NA, sipp08_2$years_us)
sipp08_2$tage <- replace(sipp08_2$tage, sipp08_2$tage == "Less than 1 full year old", 0)
sipp08_2$age <- as.numeric(sipp08_2$tage)
sipp08_2$undocu_likely <- replace(sipp08_2$undocu_likely, sipp08_2$armed_forces==1 | sipp08_2$social_security==1, 0 )
sipp08_2$undocu_logical <- as.factor(ifelse(sipp08_2$citizen==0 & (sipp08_2$armed_forces==0 | sipp08_2$medicare==0 | sipp08_2$social_security==0), 1, 0))



# Define column sets for different analyses
variable_lists <- list(
  base = c("undocu_likely", "central_latino", "bpl_asia", "medicaid", "age", "fem", 
           "married", "cit_spouse", "nonfluent", "spanish_hispanic_latino", 
           "household_size", "poverty", "asian", "black", "white", "other_race", 
           "employed", "years_us", "yrsed"),
  
  descriptive = c("undocu_likely", "undocu_logical", "bpl_foreign", "medicaid", 
                  "central_latino", "bpl_asia", "age", "fem", "married", "cit_spouse", 
                  "nonfluent", "spanish_hispanic_latino", "household_size", "poverty", 
                  "asian", "black", "white", "other_race", "employed", "years_us", "yrsed")

)

# Create analysis datasets
create_datasets <- function(data, variables) {
  datasets <- list()
  
  # Filter for undocu_logical == 1
  undocu_college_filter <- data[data$undocu_logical == 1 & data$college == 1, ]
  
  datasets$dTable <- as.data.frame(undocu_college_filter[, variables$descriptive]) %>%
    mutate_at(vars(-undocu_likely), as.numeric) %>%
    na.omit()
  
  datasets$sample <- as.data.frame(undocu_college_filter[, variables$base]) %>%
    na.omit()
  
  datasets$knn <- as.data.frame(undocu_college_filter[, variables$base]) %>%
    mutate_at(vars(-undocu_likely), as.numeric) %>%
    na.omit()
  
  
  # Demographic subsets (college graduates only)
  datasets$noncit <- data[data$citizen == 0 & data$college == 1, ]
  datasets$central_latino <- data[data$central_latino == 1 & data$college == 1, ]
  datasets$spanish_hispanic_latino <- data[data$spanish_hispanic_latino == 1 & data$college == 1, ]
  datasets$top_states <- data[data$top_ten_states == 1 & data$college == 1, ]
  
  return(datasets)
}


# Create all analysis datasets
all_data <- create_datasets(sipp08_2, variable_lists)



View(sipp08_2)

setwd("G:/Shared drives/Undocu Research/Data")
write.csv(sipp08_2, "(Graduates)SIPP08_2.csv", row.names = FALSE)
```

## Logistic model

```{r}
library(stargazer)
library(caret)
library(xtable)


levels(all_data$sample$undocu_likely) <- make.names(levels(all_data$sample$undocu_likely))


set.seed(1)
train_index_logistic <- createDataPartition(all_data$sample$undocu_likely, p = 0.7, list = FALSE)
train_logistic <- all_data$sample[train_index_logistic, ]
test_logistic <- all_data$sample[-train_index_logistic, ]

## Create trainControl object
control <- trainControl(
    method = "cv",
    number = 10,  
    summaryFunction = twoClassSummary,
    classProbs = TRUE,
    sampling = "up"
)

## Train glm with custom trainControl
logistic_model <- train(undocu_likely ~ age + fem + married + cit_spouse + nonfluent + spanish_hispanic_latino + central_latino + bpl_asia + medicaid + household_size + poverty + asian + black + white + other_race + employed + years_us + yrsed, train_logistic,
               method = "glm",
               trControl = control,
               metric = 'ROC')

p_logistic <- predict(logistic_model, test_logistic)


# Generate confusion matrix
logistic_matrix <- confusionMatrix(p_logistic, test_logistic$undocu_likely, positive = "X1")
print(logistic_matrix)
summary_logistic<-summary(logistic_model)$coefficients[,c(1,4)]
summary_logistic
xtable(summary_logistic, digits=4)


## Metrics for Figure 2 comparison
# Make predictions on test set
logistic.preds = predict(logistic_model, newdata = test_logistic, type = "prob")[, 2]  # Get probabilities for class "1"
# Create prediction object for ROCR
logistic.prediction = prediction(logistic.preds, test_logistic$undocu_likely)

logistic.pr = performance(logistic.prediction,"prec","rec") # Precision-Recall curve
```

## K-Nearest Neighbors (KNN) model

```{r}
library(class)
library(caTools)

set.seed(1)
levels(all_data$knn$undocu_likely) <- make.names(levels(all_data$knn$undocu_likely))
train_index_knn <- createDataPartition(all_data$knn$undocu_likely, p = 0.7, list = FALSE)
train_knn <- all_data$knn[train_index_knn, ]
test_knn <- all_data$knn[-train_index_knn, ]



knn_model <- train(undocu_likely ~., data = train_knn, method = "knn", 
                       trControl = control, 
                       tuneLength = 10,
                       metric = 'ROC',
                       preProcess = c('center', 'scale'))

knn_model

predict_knn <- predict(knn_model, test_knn)

knn_matrix <- confusionMatrix(predict_knn, test_knn$undocu_likely, positive = "X1")
print(knn_matrix)


## Metrics for Figure 2 comparison
# Make predictions on test set
knn.preds = predict(knn_model, newdata = test_knn, type = "prob")[, 2]  # Get probabilities for class "1"
# Create prediction object for ROCR
knn.prediction = prediction(knn.preds, test_knn$undocu_likely)

knn.pr = performance(knn.prediction,"prec","rec") # Precision-Recall curve
```

## Random Forest (RF) model

```{r}
library(class)
library(caTools)
library(caret)
library(rpart)  ## recursive partitioning

levels(all_data$sample$undocu_likely) <- make.names(levels(all_data$sample$undocu_likely))


train_index_rf <- createDataPartition(all_data$sample$undocu_likely, p = 0.7, list = FALSE)
train_rf <- all_data$sample[train_index_rf, ]
test_rf <- all_data$sample[-train_index_rf, ]


train_rf <- train_rf %>%
  select(-undocu_likely, undocu_likely)

#Manual search by create 10 folds and repeat 3 times
#control_rf <- trainControl(method = 'repeatedcv',
                        #number = 10,
                       # repeats = 3,
                        #search = 'grid')

tunegrid <- expand.grid(mtry = c(2,4,8,12),
                      splitrule = c("gini", "extratrees"),
                      min.node.size = 1)

control_up <- trainControl(
    method = "cv",
    number = 10,  
    summaryFunction = twoClassSummary,
    classProbs = TRUE,
    sampling = "up",
)


set.seed(1)
rf_model <- train(undocu_likely ~ age + fem + married + cit_spouse + medicaid + nonfluent + spanish_hispanic_latino + central_latino + bpl_asia + household_size + poverty + asian + black + white + other_race + employed + years_us + yrsed,
               data = train_rf,
               method = "ranger",
               trControl = control_up,
               tuneLength = 5,
               importance = "impurity",
               metric = 'ROC')
print(rf_model)
plot(rf_model)

p_rf <- predict(rf_model, test_rf)

rf_matrix <- confusionMatrix(p_rf, test_rf$undocu_likely, positive="X1")
rf_matrix


library(vip)
library(gridExtra)

feature_importance <- vip(rf_model, num_features = 19, bar = FALSE)
grid.arrange(feature_importance, nrow = 1)

all_data$dTable$undocu_logit <- predict(logistic_model, all_data$sample)
all_data$dTable$undocu_knn <- predict(knn_model, all_data$knn)
all_data$dTable$undocu_rf <- predict(rf_model, all_data$sample)
setwd("G:/Shared drives/Undocu Research/Data")
write.csv(all_data$dTable, "SIPP_dTable.csv", row.names = FALSE)


rf_model$finalModel$num.trees

## Metrics for Figure 2 comparison
# Make predictions on test set
rf.preds = predict(rf_model, newdata = test_rf, type = "prob")[, 2]  # Get probabilities for class "1"
# Create prediction object for ROCR
rf.prediction = prediction(rf.preds, test_rf$undocu_likely)

rf.pr = performance(rf.prediction,"prec","rec") # Precision-Recall curve
```

## Gradient-Boosted (GB) model

```{r}
library(gbm)      # For Gradient Boosting
library(class)
library(caTools)
library(caret)
library(ROCR)

## Data partitioning for caret library
train_index_gbm <- createDataPartition(all_data$sample$undocu_likely, p = 0.7, list = FALSE)
train_gbm <- all_data$sample[train_index_gbm, ]
test_gbm <- all_data$sample[-train_index_gbm, ]

## Data partitioning for gbm library
all_data$sample_num <- all_data$sample
all_data$sample_num$undocu_likely_num <- as.numeric(all_data$sample_num$undocu_likely) - 1
train_index_gbm_num <- createDataPartition(all_data$sample_num$undocu_likely_num, p = 0.7, list = FALSE)
train_gbm_num <- all_data$sample_num[train_index_gbm_num, ]
test_gbm_num <- all_data$sample_num[-train_index_gbm_num, ]
                   

caret_gbm_model <- train(
  undocu_likely ~ age + fem + married + cit_spouse + medicaid + nonfluent + 
    spanish_hispanic_latino + central_latino + bpl_asia + household_size + 
    poverty + asian + black + white + other_race + employed + years_us + yrsed,
  data = train_gbm,
  method = "gbm",
  trControl = control_up,
  metric = "ROC",                       # MAXIMIZE ROC (this is the key!)
  verbose = FALSE
)

gbm_model <- gbm(undocu_likely_num ~ age + fem + married + cit_spouse + medicaid + nonfluent + spanish_hispanic_latino + central_latino + bpl_asia + household_size + poverty + asian + black + white + other_race + employed + years_us + yrsed, data = train_gbm_num, 
                 verbose = FALSE,
                 cv.folds = 10,
                 n.trees = 1766,
                 shrinkage = 0.005, #Optimimal
                 interaction.depth = 6)

## Model summary and optimal number of trees
caret_gbm_model
print(caret_gbm_model$bestTune)
optimal_trees_gbm <- caret_gbm_model$bestTune$n.trees

gbm_model
print(gbm_model$bestTune)
optimal_trees_gbm_two <- gbm_model$bestTune$n.trees
```

## GBM Performance

```{r}
# Make predictions on test set
boost.pred = predict(caret_gbm_model, newdata = test_gbm, type = "prob", n.trees = optimal_trees_gbm)[, 2]  # Get probabilities for class "1"
# Create prediction object for ROCR
boost.prediction = prediction(boost.pred, test_gbm$undocu_likely)
# Get raw predictions (class labels)
boost.predict = predict(caret_gbm_model, newdata = test_gbm, n.trees=optimal_trees_gbm)  # Class predictions

boost.roc = performance(boost.prediction,"tpr","fpr")   # ROC curve 
boost.pr = performance(boost.prediction,"prec","rec") # Precision-Recall curve
boost.auc <- performance(boost.prediction,"auc")@y.values[[1]] # AUC value

boost_auc_pr <- trapz(
  as.vector(boost.pr@x.values)[[1]][-1], 
  as.vector(boost.pr@y.values)[[1]][-1]
)

gbm_performance <- boost.pr
plot(boost.roc, main = "ROC Curve", col = "blue", lwd = 2) #PLOT EACH ON A SINGLE GRAPH FOR ROC
abline(a = 0, b = 1, lty = 2, col = "red")
plot(boost.pr) # plot Precision-Recall curve


cat("Optimal number of trees:", optimal_trees_gbm, "\n")
cat("AUC-ROC:", round(boost.auc, 4), "\n")
cat("AUC-PR:", round(boost_auc_pr, 4), "\n")

predicted_class <- ifelse(boost.pred > 0.5, 1, 0)
table(Predicted = predicted_class, Actual = test_gbm$undocu_likely)
plot(caret_gbm_model)

# Feature Importance
library(vip)
library(gridExtra)

feature_importance_gbm <- vip(caret_gbm_model, num_features = 19, bar = FALSE)
grid.arrange(feature_importance_gbm, nrow = 1)

## Metrics for Figure 2 comparison (gbm library model)
# Make predictions on test set
gbm.preds = predict(gbm_model, newdata = test_gbm_num, type = "response", n.trees = optimal_trees_gbm_two)
# Create prediction object for ROCR
gbm.prediction = prediction(gbm.preds, test_gbm_num$undocu_likely)

gbm.pr = performance(gbm.prediction,"prec","rec") # Precision-Recall curve
```

## GBM P-Quartiles and Thresholds

```{r}
test_gbm$p = predict(caret_gbm_model, newdata = test_gbm, type = "prob", n.trees = optimal_trees_gbm)[, 2]
test_gbm_num$p = predict(gbm_model, newdata = test_gbm_num, type = "response", n.trees = optimal_trees_gbm_two)


# Create quartiles based on probability magnitude
test_gbm$p_quartiles <- cut(test_gbm$p, 
                                   breaks = quantile(test_gbm$p, probs = c(0, 0.25, 0.5, 0.75, 1)), 
                                   labels = c("Q1", "Q2", "Q3", "Q4"), 
                                   include.lowest = TRUE)

test_gbm_num$p_quartiles <- cut(test_gbm_num$p, 
                                   breaks = quantile(test_gbm_num$p, probs = c(0, 0.25, 0.5, 0.75, 1)), 
                                   labels = c("Q1", "Q2", "Q3", "Q4"), 
                                   include.lowest = TRUE)
# Check the distributions
table(test_gbm$p_quartiles)
table(test_gbm_num$p_quartiles)

# Proportion of undocu_likely in each quartiles
quartiles_table <- test_gbm %>%
  group_by(p_quartiles) %>%
  summarise(
    count = sum(undocu_likely == "X1"),
    n = n(),
    actual_proportion = (count / n)
    )
quartiles_table_gbm <- test_gbm_num %>%
  group_by(p_quartiles) %>%
  summarise(
    count = sum(undocu_likely == "X1"),
    n = n(),
    actual_proportion = (count / n)
    )

xtable(quartiles_table, digits=4)
print(quartiles_table)
xtable(quartiles_table_gbm, digits=4)
print(quartiles_table_gbm)


top_75_sample <- test_gbm %>%
  filter(undocu_likely == "X1") %>%
  arrange(desc(p)) %>%
  mutate(
    rank = row_number(),
    cumulative_p = rank / n(),
    top_75 = cumulative_p <= 0.75
  ) %>%
  filter(top_75==TRUE)
threshold_75 <- min(top_75_sample$p)


test_gbm$prediction <- ifelse(test_gbm$p >= threshold_75, 1, 0)
test_gbm_num$predicted_class <- ifelse(test_gbm_num$p >= threshold_75, 1, 0)
```

## Figure 2 (Precision-Recall Curves)

```{r}
setwd("G:/Shared drives/Undocu Research/Output/Figures")
# ==============================================================================
# PREPARE PRECISION-RECALL DATA FOR ALL MODELS
# ==============================================================================

# Start with boosted trees as baseline
boost_pr_data <- data.frame(
  recall = unlist(boost.pr@x.values), 
  precision = unlist(boost.pr@y.values)
)

# Interpolate other models to match boosted trees recall values
gbm_pr_data <- data.frame(
  recall = approx(unlist(gbm.pr@x.values), unlist(gbm.pr@y.values), 
                 xout = boost_pr_data$recall)$x,
  precision = approx(unlist(gbm.pr@x.values), unlist(gbm.pr@y.values), 
                    xout = boost_pr_data$recall)$y
)

# Interpolate other models to match boosted trees recall values
rf_pr_data <- data.frame(
  recall = approx(unlist(rf.pr@x.values), unlist(rf.pr@y.values), 
                 xout = boost_pr_data$recall)$x,
  precision = approx(unlist(rf.pr@x.values), unlist(rf.pr@y.values), 
                    xout = boost_pr_data$recall)$y
)

logistic_pr_data <- data.frame(
  recall = approx(unlist(logistic.pr@x.values), unlist(logistic.pr@y.values), 
                 xout = boost_pr_data$recall)$x,
  precision = approx(unlist(logistic.pr@x.values), unlist(logistic.pr@y.values), 
                    xout = boost_pr_data$recall)$y
)

knn_pr_data <- data.frame(
  recall = approx(unlist(knn.pr@x.values), unlist(knn.pr@y.values), 
                 xout = boost_pr_data$recall)$x,
  precision = approx(unlist(knn.pr@x.values), unlist(knn.pr@y.values), 
                    xout = boost_pr_data$recall)$y
)

# ==============================================================================
# COMBINE ALL DATA
# ==============================================================================

all_pr_data <- boost_pr_data

# Add precision values for other models
all_pr_data$gbm_precision <- gbm_pr_data$precision
all_pr_data$rf_precision <- rf_pr_data$precision
all_pr_data$logistic_precision <- logistic_pr_data$precision
all_pr_data$knn_precision <- knn_pr_data$precision

# Calculate precision differences relative to boosted trees
all_pr_data$gbm_precision_difference <- gbm_pr_data$precision - all_pr_data$precision
all_pr_data$rf_precision_difference <- rf_pr_data$precision - all_pr_data$precision
all_pr_data$logistic_precision_difference <- logistic_pr_data$precision - all_pr_data$precision
all_pr_data$knn_precision_difference <- knn_pr_data$precision - all_pr_data$precision

# ==============================================================================
# PLOT 1: PRECISION-RECALL CURVES FOR ALL MODELS
# ==============================================================================

#pdf("undocumented_pr_curves.pdf", family="Times", width=9)
png("(graduates) undocumented_pr_curves.png", width = 22*100, height = 16*100, res = 300, family = "Times")

op <- par(family = "serif")

plot(all_pr_data$recall, all_pr_data$precision, type = "l", pch = 19,
     col = "black", xlab = "Recall", ylab = "Precision",
     lwd = 3, ylim=c(0, 1), xlim = c(0,1), lty = 1, cex.lab=1.5, cex.axis=1)

lines(all_pr_data$recall, all_pr_data$gbm_precision, type = "l", 
      col = "yellow", lwd = 3, lty = 2)
lines(all_pr_data$recall, all_pr_data$rf_precision, type = "l", 
      col = "blue", lwd = 3, lty = 3)
lines(all_pr_data$recall, all_pr_data$logistic_precision, type = "l",
      col = "red", lwd = 3, lty = 4)
lines(all_pr_data$recall, all_pr_data$knn_precision, type = "l",
      col = "green", lwd = 3, lty = 5)

legend(x = 0.5, y = 1, 
       col = c("white", "black", "yellow", "blue", "red", "green"),
       lwd = c(3, 3, 3, 3, 3, 3), 
       lty = c(0, 1, 2, 3, 4, 5),
       legend = c("Model:", "Boosted Trees", "GBM Library", "Random Forest", "Logistic", "KNN"),
       bty = 'n', cex = 1.25, ncol = 1, seg.len = 6)

# Add random baseline
random_baseline <- total_undocu / nrow(test_gbm)
abline(h = random_baseline, lty = 2, col = "gray")

dev.off()
```
