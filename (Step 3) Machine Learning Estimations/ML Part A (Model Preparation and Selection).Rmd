---
title: "ML Undocu imputation (SIPP to ACS)"
author: "Mario Arce Acosta"
date: "2025-03-17"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Load raw data}
data_path <- "G:/Shared drives/Undocu Research/Data"
setwd(data_path)

library(readr)
sipp08_2 <- read_csv("(Step 1 output) Core_TM SIPP 2008 Wave 2.csv")
View(sipp08_2)
```

```{r Data preparation}
library(dplyr)
sipp08_2 <- sipp08_2 %>%
  mutate(
    undocu_entry = as.factor(ifelse(timstat=="Other", 1, 0)),
    undocu_likely = as.factor(ifelse(timstat=="Other" & eadjust=="No", 1, 0)),
    education = case_when(
      eeducate == "10th Grade"  | eeducate == "11th Grade" | eeducate == "12th grade, no diploma" | eeducate == "1st, 2nd, 3rd, or 4th grade" | eeducate == "5th Or 6th Grade" | eeducate == "7th Or 8th Grade" | eeducate == "9th Grade" | eeducate == "Less Than 1st Grade"~ "No HS diploma",
      eeducate == "Diploma or certificate from a" | eeducate == "High School Graduate - (diploma" ~ "HS diploma",
      eeducate == "Some college, but no degree" ~ "Some college",
      eeducate =="Associate (2-yr) college degree" ~ "Associate's",
      eeducate == "Bachelor's degree (for example:" ~ "Bachelor's",
      eeducate == "Master's degree (For example: MA," ~ "Master's",
      eeducate == "Doctorate degree (for example:" ~ "PhD",
      TRUE ~ "Unknown" # Default case
    ),
    yrsed = case_when(
      eeducate == "10th Grade"~10,
      eeducate == "11th Grade"~11,
      eeducate == "12th grade, no diploma" | eeducate == "Diploma or certificate from a" | eeducate == "High School Graduate - (diploma" | eeducate == "Some college, but no degree" ~12,
      eeducate == "1st, 2nd, 3rd, or 4th grade"~2.5,
      eeducate == "5th Or 6th Grade"~5.5,
      eeducate == "7th Or 8th Grade"~7.5,
      eeducate == "9th Grade"~9,
      eeducate == "Less Than 1st Grade"~0,
      eeducate =="Associate (2-yr) college degree"~14,
      eeducate == "Bachelor's degree (for example:"~16,
      eeducate == "Master's degree (For example: MA,"~17.5,
      eeducate == "Doctorate degree (for example:"~22,
      eeducate == "Professional School degree (for"~16,
      TRUE ~ NA
    ),
    college = as.factor(ifelse(eeducate=="Bachelor's degree (for example:" | eeducate=="Master's degree (For example: MA," | eeducate=="Doctorate degree (for example:", 1, 0)),
    hs_only = as.factor(ifelse(eeducate=="Some college, but no degree" | eeducate== "Associate (2-yr) college degree" | eeducate=="High School Graduate - (diploma" | eeducate=="Diploma or certificate from a", 1, 0)),
    immig_yr = case_when(
      tmoveus == "1961"~1961,
      tmoveus == "1961-1968"~1966,
      tmoveus == "1969-1973"~1971,
      tmoveus == "1974-1978"~1976,
      tmoveus == "1979-1980"~1980,
      tmoveus == "1981-1983"~1982,
      tmoveus == "1984-1985"~1984,
      tmoveus == "1986-1988"~1987,
      tmoveus == "1989-1990"~1989,
      tmoveus == "1991-1992"~1991,
      tmoveus == "1993-1994"~1993,
      tmoveus == "1995-1996"~1995,
      tmoveus == "1997-1998"~1998,
      tmoveus == "1999"~1999,
      tmoveus == "2000"~2000,
      tmoveus == "2001"~2001,
      tmoveus == "2002-2003"~2002,
      tmoveus == "2004"~2004,
      tmoveus == "2005"~2005,
      tmoveus == "2006"~2006,
      tmoveus == "2007"~2007,
      tmoveus == "2008-2009"~2009,
      TRUE ~ 0 # Default case
    ),
    married = as.factor(ifelse(ems=="Married, spouse absent" | ems=="Married, spouse present", 1, 0)),
    english_difficult = as.factor(ifelse(ehowwell=="Not at all" | ehowwell=="Not well", 1, 0)),
    nonfluent = as.factor(ifelse(ehowwell=="Not at all" | ehowwell=="Not well", 1, 0)),
    english_home = as.factor(ifelse(tlang1=="Not in Universe", 1, 0)),
    spanish_hispanic_latino = as.factor(ifelse(eorigin=="Yes", 1, 0)),
    medicaid = as.factor(ifelse(rcutyp57=="Yes, covered", 1, 0)),
    household_size = ehhnumpp,
    race = case_when(
      erace=="Asian alone" ~ "Asian",
      erace=="Black alone" ~ "Black",
      erace=="White alone" ~ "White",
      erace=="Residual" ~ "Other",
      TRUE ~ "Unknown"
    ),
    fem = as.factor(ifelse(esex=="Female", 1, 0)),
    asian = as.factor(ifelse(erace=="Asian alone", 1, 0)),
    black = as.factor(ifelse(erace=="Black alone", 1, 0)),
    white = as.factor(ifelse(erace=="White alone", 1, 0)),
    other_race = as.factor(ifelse(erace=="Residual", 1, 0)),
    employed = as.factor(ifelse(rmesr=="With a job at least 1 but not all" | rmesr=="With a job entire month, absent" | rmesr=="With a job entire month, worked", 1, 0)),
    years_us = rhcalyr - immig_yr,
    citizen = as.factor(ifelse(ecitizen=="Yes", 1, 0)),
    cit_spouse = as.factor(cit_spouse),
    poverty = as.factor(ifelse(thearn<rhpov, 1, 0)),
    armed_forces = as.factor(ifelse(eafnow=="Yes" | eafever=="Yes", 1, 0)),
    health_ins= as.factor(ifelse(rcutyp57=="Yes, covered" | rcutyp58=="Yes, covered" , 1, 0)),
    medicare = as.factor(ifelse(ecrmth=="Yes, covered", 1, 0)),
    social_security = as.factor(ifelse(rcutyp01=="Yes, covered" | rcutyp03=="Yes, covered", 1, 0)),
    central_latino = as.factor(ifelse(tbrstate=="Central America" & eorigin=="Yes", 1, 0)),
    bpl_usa = as.factor(ifelse(ebornus=="Yes", 1, 0)),
    bpl_asia = as.factor(ifelse(tbrstate == "Eastern Asia"| tbrstate == "South Central Asia"| tbrstate == "South East Asia, West Asia,", 1, 0)),
    top_ten_states = as.factor(ifelse(tfipsst=="California" | tfipsst=="Texas" | tfipsst=="Florida" | tfipsst=="New Jersey" | tfipsst=="Illinois" | tfipsst=="New York" | tfipsst=="North Carolina" | tfipsst=="Georgia" | tfipsst=="Washington" | tfipsst=="Arizona", 1, 0))
  )

sipp08_2$bpl_foreign <- as.factor(ifelse(sipp08_2$bpl_usa==1, 0, 1))
sipp08_2$undocu_likely <- replace(sipp08_2$undocu_likely, sipp08_2$immig_yr <= 1961, 0)
sipp08_2$years_us <- ifelse(sipp08_2$years_us == 2008 | sipp08_2$years_us == 2009 | sipp08_2$years_us == -1 , NA, sipp08_2$years_us)
sipp08_2$tage <- replace(sipp08_2$tage, sipp08_2$tage == "Less than 1 full year old", 0)
sipp08_2$age <- as.numeric(sipp08_2$tage)
sipp08_2$undocu_likely <- replace(sipp08_2$undocu_likely, sipp08_2$armed_forces==1 | sipp08_2$social_security==1, 0 )
sipp08_2$undocu_logical <- as.factor(ifelse(sipp08_2$citizen==0 & (sipp08_2$armed_forces==0 | sipp08_2$medicare==0 | sipp08_2$social_security==0), 1, 0))
sipp08_2$id <- seq_len(nrow(sipp08_2))



# Define column sets for different analyses
variable_lists <- list(
  base = c("undocu_likely", "central_latino", "bpl_asia", "medicaid", "age", "fem", 
           "married", "cit_spouse", "nonfluent", "spanish_hispanic_latino", 
           "household_size", "poverty", "asian", "black", "white", "other_race", 
           "employed", "years_us", "yrsed"),
  
  descriptive = c("undocu_likely", "undocu_logical", "bpl_foreign", "medicaid", 
                  "central_latino", "bpl_asia", "age", "fem", "married", "cit_spouse", 
                  "nonfluent", "spanish_hispanic_latino", "household_size", "poverty", 
                  "asian", "black", "white", "other_race", "employed", "years_us", "yrsed")

)

# Create analysis datasets
create_datasets <- function(data, variables) {
  datasets <- list()
  
  # Filter for undocu_logical == 1
  undocu_filter <- data[data$undocu_logical == 1, ]
  
  datasets$dTable <- as.data.frame(undocu_filter[, variables$descriptive]) %>%
    mutate_at(vars(-undocu_likely), as.numeric) %>%
    na.omit()
  
  datasets$sample <- as.data.frame(undocu_filter[, variables$base]) %>%
    na.omit()
  
  datasets$knn <- as.data.frame(undocu_filter[, variables$base]) %>%
    mutate_at(vars(-undocu_likely), as.numeric) %>%
    na.omit()
  
  
  # Demographic subsets (college graduates only)
  datasets$noncit <- data[data$citizen == 0 & data$college == 1, ]
  datasets$central_latino <- data[data$central_latino == 1 & data$college == 1, ]
  datasets$spanish_hispanic_latino <- data[data$spanish_hispanic_latino == 1 & data$college == 1, ]
  datasets$top_states <- data[data$top_ten_states == 1 & data$college == 1, ]
  
  return(datasets)
}


# Create all analysis datasets
all_data <- create_datasets(sipp08_2, variable_lists)



View(sipp08_2)

setwd("G:/Shared drives/Undocu Research/Data")
write.csv(sipp08_2, "SIPP08_2.csv", row.names = FALSE)
```

# Extract dataframes from list to separate objects

sipp08_2_dTable \<- all_data$dTable
sipp08_2_knn <- all_data$knn sipp08_2_sample \<- all_data$rf
sipp08_2_noncit <- all_data$noncit sipp08_2_central_latino \<-
all_data$central_latino
sipp08_2_spanish_hispanic_latino <- all_data$spanish_hispanic_latino
sipp08_2_top_states \<- all_data\$top_states

# Logistic

```{r Logistic Regression}
library(stargazer)
library(caret)
library(xtable)


levels(all_data$sample$undocu_likely) <- make.names(levels(all_data$sample$undocu_likely))


set.seed(1)
train_index_logistic <- createDataPartition(all_data$sample$undocu_likely, p = 0.7, list = FALSE)
train_logistic <- all_data$sample[train_index_logistic, ]
test_logistic <- all_data$sample[-train_index_logistic, ]

## Create trainControl object
control <- trainControl(
    method = "cv",
    number = 10,  
    summaryFunction = twoClassSummary,
    classProbs = TRUE,
    sampling = "up"
)

## Train glm with custom trainControl
logistic_model <- train(undocu_likely ~ age + fem + married + cit_spouse + nonfluent + spanish_hispanic_latino + central_latino + bpl_asia + medicaid + household_size + poverty + asian + black + white + other_race + employed + years_us + yrsed, train_logistic,
               method = "glm",
               trControl = control,
               metric = 'ROC')

p_logistic <- predict(logistic_model, test_logistic)


# Generate confusion matrix
logistic_matrix <- confusionMatrix(p_logistic, test_logistic$undocu_likely, positive = "X1")
print(logistic_matrix)
summary_logistic<-summary(logistic_model)$coefficients[,c(1,4)]
summary_logistic
xtable(summary_logistic, digits=4)


## Metrics for Figure 2 comparison
# Make predictions on test set
logistic.preds = predict(logistic_model, newdata = test_logistic, type = "prob")[, 2]  # Get probabilities for class "1"
# Create prediction object for ROCR
logistic.prediction = prediction(logistic.preds, test_logistic$undocu_likely)

logistic.pr = performance(logistic.prediction,"prec","rec") # Precision-Recall curve
```

# KNN

```{r KNN}
library(class)
library(caTools)

set.seed(1)
levels(all_data$knn$undocu_likely) <- make.names(levels(all_data$knn$undocu_likely))
train_index_knn <- createDataPartition(all_data$knn$undocu_likely, p = 0.7, list = FALSE)
train_knn <- all_data$knn[train_index_knn, ]
test_knn <- all_data$knn[-train_index_knn, ]



knn_model <- train(undocu_likely ~., data = train_knn, method = "knn", 
                       trControl = control, 
                       tuneLength = 10,
                       metric = 'ROC',
                       preProcess = c('center', 'scale'))

knn_model

predict_knn <- predict(knn_model, test_knn)

knn_matrix <- confusionMatrix(predict_knn, test_knn$undocu_likely, positive = "X1")
print(knn_matrix)


## Metrics for Figure 2 comparison
# Make predictions on test set
knn.preds = predict(knn_model, newdata = test_knn, type = "prob")[, 2]  # Get probabilities for class "1"
# Create prediction object for ROCR
knn.prediction = prediction(knn.preds, test_knn$undocu_likely)

knn.pr = performance(knn.prediction,"prec","rec") # Precision-Recall curve
```

-   The number of trees in the forest
-   The number of features to consider at any given split: $m_{try}$
-   The complexity of each tree
-   The sampling scheme
-   The splitting rule to use during tree construction
-   and (2) typically have the largest impact on predictive accuracy and
    should always be tuned. (3) and (4) tend to have marginal impact on
    predictive accuracy but are still worth exploring. They also have
    the ability to influence computational efficiency. (5) tends to have
    the smallest impact on predictive accuracy and is used primarily to
    increase computational efficiency. \# RF

```{r Random Forest}
library(class)
library(caTools)
library(caret)
library(rpart)  ## recursive partitioning

levels(all_data$sample$undocu_likely) <- make.names(levels(all_data$sample$undocu_likely))


train_index_rf <- createDataPartition(all_data$sample$undocu_likely, p = 0.7, list = FALSE)
train_rf <- all_data$sample[train_index_rf, ]
test_rf <- all_data$sample[-train_index_rf, ]


train_rf <- train_rf %>%
  select(-undocu_likely, undocu_likely)

#Manual search by create 10 folds and repeat 3 times
#control_rf <- trainControl(method = 'repeatedcv',
                        #number = 10,
                       # repeats = 3,
                        #search = 'grid')

tunegrid <- expand.grid(mtry = c(2,4,8,12),
                      splitrule = c("gini", "extratrees"),
                      min.node.size = 1)

control_up <- trainControl(
    method = "cv",
    number = 10,  
    summaryFunction = twoClassSummary,
    classProbs = TRUE,
    sampling = "up",
)


set.seed(1)
rf_model <- train(undocu_likely ~ age + fem + married + cit_spouse + medicaid + nonfluent + spanish_hispanic_latino + central_latino + bpl_asia + household_size + poverty + asian + black + white + other_race + employed + years_us + yrsed,
               data = train_rf,
               method = "ranger",
               trControl = control_up,
               tuneLength = 5,
               importance = "impurity",
               metric = 'ROC')
print(rf_model)
plot(rf_model)

p_rf <- predict(rf_model, test_rf)

rf_matrix <- confusionMatrix(p_rf, test_rf$undocu_likely, positive="X1")
rf_matrix


library(vip)
library(gridExtra)

feature_importance <- vip(rf_model, num_features = 19, bar = FALSE)
grid.arrange(feature_importance, nrow = 1)

all_data$dTable$undocu_logit <- predict(logistic_model, all_data$sample)
all_data$dTable$undocu_knn <- predict(knn_model, all_data$knn)
all_data$dTable$undocu_rf <- predict(rf_model, all_data$sample)
setwd("G:/Shared drives/Undocu Research/Data")
write.csv(all_data$dTable, "SIPP_dTable.csv", row.names = FALSE)


rf_model$finalModel$num.trees

## Metrics for Figure 2 comparison
# Make predictions on test set
rf.preds = predict(rf_model, newdata = test_rf, type = "prob")[, 2]  # Get probabilities for class "1"
# Create prediction object for ROCR
rf.prediction = prediction(rf.preds, test_rf$undocu_likely)

rf.pr = performance(rf.prediction,"prec","rec") # Precision-Recall curve
```

all_data$sample_num <- all_data$sample
all_data$sample_num$undocu_likely_num \<-
as.numeric(all_data$sample_num$undocu_likely) - 1

train_index_gbm \<-
createDataPartition(all_data$sample_num$undocu_likely_num, p = 0.7, list
= FALSE) train_gbm \<- all_data$sample_num[train_index_gbm, ]
test_gbm <- all_data$sample_num[-train_index_gbm, ]

gbm_model \<- gbm( undocu_likely_num \~ age + fem + married +
cit_spouse + medicaid + nonfluent + spanish_hispanic_latino +
central_latino + bpl_asia + household_size + poverty + asian + black +
white + other_race + employed + years_us + yrsed, data = train_gbm,
verbose = FALSE, cv.folds = 10, n.trees = 1766, shrinkage = 0.005,
interaction.depth = 6)

optimal_trees \<- gbm.perf(gbm_model, method = "cv", plot.it = FALSE)

# GBM

```{r Gradient-Boosting}
library(gbm)      # For Gradient Boosting
library(class)
library(caTools)
library(caret)
library(ROCR)

## Data partitioning for caret library
train_index_gbm <- createDataPartition(all_data$sample$undocu_likely, p = 0.7, list = FALSE)
train_gbm <- all_data$sample[train_index_gbm, ]
test_gbm <- all_data$sample[-train_index_gbm, ]

## Data partitioning for gbm library
all_data$sample_num <- all_data$sample
all_data$sample_num$undocu_likely_num <- as.numeric(all_data$sample_num$undocu_likely) - 1
train_index_gbm_num <- createDataPartition(all_data$sample_num$undocu_likely_num, p = 0.7, list = FALSE)
train_gbm_num <- all_data$sample_num[train_index_gbm_num, ]
test_gbm_num <- all_data$sample_num[-train_index_gbm_num, ]
                   

caret_gbm_model <- train(
  undocu_likely ~ age + fem + married + cit_spouse + medicaid + nonfluent + 
    spanish_hispanic_latino + central_latino + bpl_asia + household_size + 
    poverty + asian + black + white + other_race + employed + years_us + yrsed,
  data = train_gbm,
  method = "gbm",
  trControl = control_up,
  metric = "ROC",                       # MAXIMIZE ROC (this is the key!)
  verbose = FALSE
)

gbm_model <- gbm(undocu_likely_num ~ age + fem + married + cit_spouse + medicaid + nonfluent + spanish_hispanic_latino + central_latino + bpl_asia + household_size + poverty + asian + black + white + other_race + employed + years_us + yrsed, data = train_gbm_num, 
                 verbose = FALSE,
                 cv.folds = 10,
                 n.trees = 1766,
                 shrinkage = 0.005, #Optimimal
                 interaction.depth = 6)

## Model summary and optimal number of trees
caret_gbm_model
print(caret_gbm_model$bestTune)
optimal_trees_gbm <- caret_gbm_model$bestTune$n.trees

gbm_model
print(gbm_model$bestTune)
optimal_trees_gbm_two <- gbm_model$bestTune$n.trees
```

\# Make predictions on test set

gbm.preds = predict(gbm_model, newdata = test_gbm_num, type =
"response")[, 2] \# Get probabilities for class "1"

\# Create prediction object for ROCR

gbm.prediction = prediction(gbm.preds, test_gbm_num\$undocu_likely)

gbm.pr = performance(gbm.prediction,"prec","rec") \# Precision-Recall
curve

```{r GBM Performance}
# Make predictions on test set
boost.pred = predict(caret_gbm_model, newdata = test_gbm, type = "prob", n.trees = optimal_trees_gbm)[, 2]  # Get probabilities for class "1"
# Create prediction object for ROCR
boost.prediction = prediction(boost.pred, test_gbm$undocu_likely)
# Get raw predictions (class labels)
boost.predict = predict(caret_gbm_model, newdata = test_gbm, n.trees=optimal_trees_gbm)  # Class predictions

boost.roc = performance(boost.prediction,"tpr","fpr")   # ROC curve 
boost.pr = performance(boost.prediction,"prec","rec") # Precision-Recall curve
boost.auc <- performance(boost.prediction,"auc")@y.values[[1]] # AUC value

boost_auc_pr <- trapz(
  as.vector(boost.pr@x.values)[[1]][-1], 
  as.vector(boost.pr@y.values)[[1]][-1]
)

gbm_performance <- boost.pr
plot(boost.roc, main = "ROC Curve", col = "blue", lwd = 2) #PLOT EACH ON A SINGLE GRAPH FOR ROC
abline(a = 0, b = 1, lty = 2, col = "red")
plot(boost.pr) # plot Precision-Recall curve


cat("Optimal number of trees:", optimal_trees_gbm, "\n")
cat("AUC-ROC:", round(boost.auc, 4), "\n")
cat("AUC-PR:", round(boost_auc_pr, 4), "\n")

predicted_class <- ifelse(boost.pred > 0.5, 1, 0)
table(Predicted = predicted_class, Actual = test_gbm$undocu_likely)
plot(caret_gbm_model)

# Feature Importance
library(vip)
library(gridExtra)

feature_importance_gbm <- vip(caret_gbm_model, num_features = 19, bar = FALSE)
grid.arrange(feature_importance_gbm, nrow = 1)

## Metrics for Figure 2 comparison (gbm library model)
# Make predictions on test set
gbm.preds = predict(gbm_model, newdata = test_gbm_num, type = "response", n.trees = optimal_trees_gbm_two)
# Create prediction object for ROCR
gbm.prediction = prediction(gbm.preds, test_gbm_num$undocu_likely)

gbm.pr = performance(gbm.prediction,"prec","rec") # Precision-Recall curve
```

# P Quartiles and Thresholds

```{r}
library(xtable)
test_gbm$p = predict(caret_gbm_model, newdata = test_gbm, type = "prob", n.trees = optimal_trees_gbm)[, 2]
test_gbm_num$p = predict(gbm_model, newdata = test_gbm_num, type = "response", n.trees = optimal_trees_gbm_two)


# Create quartiles based on probability magnitude
test_gbm$p_quartiles <- cut(test_gbm$p, 
                                   breaks = quantile(test_gbm$p, probs = c(0, 0.25, 0.5, 0.75, 1)), 
                                   labels = c("Q1", "Q2", "Q3", "Q4"), 
                                   include.lowest = TRUE)

test_gbm_num$p_quartiles <- cut(test_gbm_num$p, 
                                   breaks = quantile(test_gbm_num$p, probs = c(0, 0.25, 0.5, 0.75, 1)), 
                                   labels = c("Q1", "Q2", "Q3", "Q4"), 
                                   include.lowest = TRUE)
# Check the distributions
table(test_gbm$p_quartiles)
table(test_gbm_num$p_quartiles)

# Proportion of undocu_likely in each quartiles
quartiles_table <- test_gbm %>%
  group_by(p_quartiles) %>%
  summarise(
    count = sum(undocu_likely == "X1"),
    n = n(),
    actual_proportion = (count / n)
    )
quartiles_table_gbm <- test_gbm_num %>%
  group_by(p_quartiles) %>%
  summarise(
    count = sum(undocu_likely == 1),
    n = n(),
    actual_proportion = (count / n)
    )

xtable(quartiles_table, digits=4)
print(quartiles_table)
xtable(quartiles_table_gbm, digits=4)
print(quartiles_table_gbm)


top_75_sample <- test_gbm %>%
  filter(undocu_likely == "X1") %>%
  arrange(desc(p)) %>%
  mutate(
    rank = row_number(),
    cumulative_p = rank / n(),
    top_75 = cumulative_p <= 0.75
  ) %>%
  filter(top_75==TRUE)
threshold_75 <- min(top_75_sample$p)

top_75_sample_num <- test_gbm_num %>%
  filter(undocu_likely == 1) %>%
  arrange(desc(p)) %>%
  mutate(
    rank = row_number(),
    cumulative_p = rank / n(),
    top_75 = cumulative_p <= 0.75
  ) %>%
  filter(top_75==TRUE)
threshold_75_num <- min(top_75_sample_num$p)

undocu_test <- test_gbm %>%
  filter(undocu_likely == "X1") %>%
  arrange(desc(p))

undocu_test_num <- test_gbm_num %>%
  filter(undocu_likely == 1) %>%
  arrange(desc(p))

# Create GBM high recall (p>threshold), high probability (75% of actual), low probability (Q1)
test_gbm$prediction <- ifelse(test_gbm$p >= threshold_75, 1, 0)
test_gbm_num$predicted_class <- ifelse(test_gbm_num$p >= threshold_75_num, 1, 0)

## Thersholds on undocu likely sample
quartile_thresholds_caret <- quantile(undocu_test$p, probs = c(0, 0.25, 0.5, 0.75, 1))
print(quartile_thresholds_caret)

quartile_thresholds_gbm <- quantile(undocu_test_num$p, probs = c(0, 0.25, 0.5, 0.75, 1))
print(quartile_thresholds_gbm)
```

# Figure 2

```{r Figure 2}
setwd(figures_path)
# ==============================================================================
# PREPARE PRECISION-RECALL DATA FOR ALL MODELS
# ==============================================================================

# Start with boosted trees as baseline
boost_pr_data <- data.frame(
  recall = unlist(boost.pr@x.values), 
  precision = unlist(boost.pr@y.values)
)

# Interpolate other models to match boosted trees recall values
gbm_pr_data <- data.frame(
  recall = approx(unlist(gbm.pr@x.values), unlist(gbm.pr@y.values), 
                 xout = boost_pr_data$recall)$x,
  precision = approx(unlist(gbm.pr@x.values), unlist(gbm.pr@y.values), 
                    xout = boost_pr_data$recall)$y
)



# ==============================================================================
# COMBINE ALL DATA
# ==============================================================================

all_pr_data <- boost_pr_data

# Add precision values for other models
all_pr_data$gbm_recall <- gbm_pr_data$recall
all_pr_data$gbm_precision <- gbm_pr_data$precision


# Calculate precision differences relative to boosted trees
all_pr_data$gbm_precision_difference <- gbm_pr_data$precision - all_pr_data$precision

# ==============================================================================
# PLOT 1: PRECISION-RECALL CURVES FOR ALL MODELS
# ==============================================================================

#pdf("undocumented_pr_curves.pdf", family="Times", width=9)
png("undocumented_pr_curves.png", width = 22*100, height = 16*100, res = 300, family = "Times")

op <- par(family = "serif")

plot(all_pr_data$recall, all_pr_data$precision, type = "l", pch = 19,
     col = "black", xlab = "Recall", ylab = "Precision",
     lwd = 3, ylim=c(0, 1), xlim = c(0,1), lty = 1, cex.lab=1.5, cex.axis=1)

lines(all_pr_data$recall, all_pr_data$gbm_precision, type = "l", 
      col = "yellow", lwd = 3, lty = 2)
lines(all_pr_data$recall, all_pr_data$rf_precision, type = "l", 
      col = "blue", lwd = 3, lty = 3)
lines(all_pr_data$recall, all_pr_data$logistic_precision, type = "l",
      col = "red", lwd = 3, lty = 4)
lines(all_pr_data$recall, all_pr_data$knn_precision, type = "l",
      col = "green", lwd = 3, lty = 5)

legend(x = 0.5, y = 1, 
       col = c("white", "black", "yellow", "blue", "red", "green"),
       lwd = c(3, 3, 3, 3, 3, 3), 
       lty = c(0, 1, 2, 3, 4, 5),
       legend = c("Model:", "Boosted Trees", "GBM Library", "Random Forest", "Logistic", "KNN"),
       bty = 'n', cex = 1.25, ncol = 1, seg.len = 6)

# Add random baseline
total_undocu <- sum(test_gbm$undocu_likely == "X1")
random_baseline <- total_undocu / nrow(test_gbm)
abline(h = random_baseline, lty = 2, col = "gray")

dev.off()
```

train with different ntree parameters set.seed(123) rf_model_2 \<-
train(undocu_likely \~ age + married + nonfluent +
spanish_hispanic_latino + central_latino + bpl_asia + household_size +
poverty + asian + black + white + other_race + employed + years_us +
yrsed, data = train_rf, method = 'ranger', metric = 'Accuracy', tuneGrid
= tunegrid, trControl = control_rf)

*undocu_likely* age *married* nonfluent *spanish_hispanic_latino*
central_latino *bpl_asia* household_size *poverty* asian *black* white
*other_race* employed *years_us* yrsed

```{r Logical edits performance}
library(xtable)
## General
FN_logical <- length(which(all_data$sample$undocu_logical==0 & all_data$sample$undocu_likely==1))
TN_logical <- length(which(all_data$sample$undocu_logical==0 & all_data$sample$undocu_likely==0))
FP_logical <- length(which(all_data$sample$undocu_logical==1 & all_data$sample$undocu_likely==0))
TP_logical <- length(which(all_data$sample$undocu_logical==1 & all_data$sample$undocu_likely==1))

specificity_logical<- TN_logical/(TN_logical+FP_logical)
sensitivity_logical<- TP_logical/(TP_logical+FN_logical)
ppv_logical <- TP_logical/(TP_logical+FP_logical)
accuracy_logical <- (TP_logical+TN_logical)/(TP_logical+TN_logical+FP_logical+FN_logical)  

## Noncitizens
FN_logical_noncit <- length(which(all_data$noncit$undocu_logical==0 & all_data$noncit$undocu_likely==1))
TN_logical_noncit <- length(which(all_data$noncit$undocu_logical==0 & all_data$noncit$undocu_likely==0))
FP_logical_noncit <- length(which(all_data$noncit$undocu_logical==1 & all_data$noncit$undocu_likely==0))
TP_logical_noncit <- length(which(all_data$noncit$undocu_logical==1 & all_data$noncit$undocu_likely==1))

specificity_logical_noncit<- TN_logical_noncit/(TN_logical_noncit+FP_logical_noncit)
sensitivity_logical_noncit<- TP_logical_noncit/(TP_logical_noncit+FN_logical_noncit)
ppv_logical_noncit <- TP_logical_noncit/(TP_logical_noncit+FP_logical_noncit)
accuracy_logical_noncit <- (TP_logical_noncit+TN_logical_noncit)/(TP_logical_noncit+TN_logical_noncit+FP_logical_noncit+FN_logical_noncit)  

## Top ten states of undocumented immigrants
#FN_logical_top_states <- length(which(sipp08_2_top_states$undocu_logical==0 & sipp08_2_top_states$undocu_likely==1))
#TN_logical_top_states <- length(which(sipp08_2_top_states$undocu_logical==0 & sipp08_2_top_states$undocu_likely==0))
#FP_logical_top_states <- length(which(sipp08_2_top_states$undocu_logical==1 & sipp08_2_top_states$undocu_likely==0))
#TP_logical_top_states <- length(which(sipp08_2_top_states$undocu_logical==1 & sipp08_2_top_states$undocu_likely==1))

#specificity_logical_top_states<- TN_logical_top_states/(TN_logical_top_states+FP_logical_top_states)
#sensitivity_logical_top_states<- TP_logical_top_states/(TP_logical_top_states+FN_logical_top_states)
#ppv_logical_top_states <- TP_logical_top_states/(TP_logical_top_states+FP_logical_top_states)
#accuracy_logical_top_states <- (TP_logical_top_states+TN_logical_top_states)/(TP_logical_top_states+TN_logical_top_states+FP_logical_top_states+FN_logical_top_states)  

## Hispanic, latino, spanish
FN_logical_spanish_hispanic_latino <- length(which(all_data$spanish_hispanic_latino$undocu_logical==0 & all_data$spanish_hispanic_latino$undocu_likely==1))
TN_logical_spanish_hispanic_latino <- length(which(all_data$spanish_hispanic_latino$undocu_logical==0 & all_data$spanish_hispanic_latino$undocu_likely==0))
FP_logical_spanish_hispanic_latino <- length(which(all_data$spanish_hispanic_latino$undocu_logical==1 & all_data$spanish_hispanic_latino$undocu_likely==0))
TP_logical_spanish_hispanic_latino <- length(which(all_data$spanish_hispanic_latino$undocu_logical==1 & all_data$spanish_hispanic_latino$undocu_likely==1))

specificity_logical_spanish_hispanic_latino<- TN_logical_spanish_hispanic_latino/(TN_logical_spanish_hispanic_latino+FP_logical_spanish_hispanic_latino)
sensitivity_logical_spanish_hispanic_latino<- TP_logical_spanish_hispanic_latino/(TP_logical_spanish_hispanic_latino+FN_logical_spanish_hispanic_latino)
ppv_logical_spanish_hispanic_latino <- TP_logical_spanish_hispanic_latino/(TP_logical_spanish_hispanic_latino+FP_logical_spanish_hispanic_latino)
accuracy_logical_spanish_hispanic_latino <- (TP_logical_spanish_hispanic_latino+TN_logical_spanish_hispanic_latino)/(TP_logical_spanish_hispanic_latino+TN_logical_spanish_hispanic_latino+FP_logical_spanish_hispanic_latino+FN_logical_spanish_hispanic_latino)  

## Central America and Latino (Central America + Mexico)
FN_logical_central_latino <- length(which(all_data$central_latino$undocu_logical==0 & all_data$central_latino$undocu_likely==1))
TN_logical_central_latino <- length(which(all_data$central_latino$undocu_logical==0 & all_data$central_latino$undocu_likely==0))
FP_logical_central_latino <- length(which(all_data$central_latino$undocu_logical==1 & all_data$central_latino$undocu_likely==0))
TP_logical_central_latino <- length(which(all_data$central_latino$undocu_logical==1 & all_data$central_latino$undocu_likely==1))


specificity_logical_central_latino<- TN_logical_central_latino/(TN_logical_central_latino+FP_logical_central_latino)
sensitivity_logical_central_latino<- TP_logical_central_latino/(TP_logical_central_latino+FN_logical_central_latino)
ppv_logical_central_latino <- TP_logical_central_latino/(TP_logical_central_latino+FP_logical_central_latino)
accuracy_logical_central_latino <- (TP_logical_central_latino+TN_logical_central_latino)/(TP_logical_central_latino+TN_logical_central_latino+FP_logical_central_latino+FN_logical_central_latino)



logical_sensitivity <- c(sensitivity_logical, sensitivity_logical_noncit, sensitivity_logical_spanish_hispanic_latino, sensitivity_logical_central_latino)

logical_specificity <- c(specificity_logical, specificity_logical_noncit, specificity_logical_spanish_hispanic_latino, specificity_logical_central_latino)

logical_ppv <- c(ppv_logical, ppv_logical_noncit, ppv_logical_spanish_hispanic_latino, ppv_logical_central_latino)

logical_accuracy <- c(accuracy_logical, accuracy_logical_noncit, accuracy_logical_spanish_hispanic_latino, accuracy_logical_central_latino)


logical_edits_table <- data.frame(logical_sensitivity, logical_specificity, logical_ppv, logical_accuracy)
logical_edits_comparison <- as.data.frame(t(logical_edits_table))
colnames(logical_edits_comparison) <- c("Initial SIPP sample", "Noncitizens", "Top 10 states", "Hispanic/Latino/Spanish", "Central America and Latino")

# Logical edits metrics (comparison of Logical edits filters)
xtable(logical_edits_comparison, digits=4)
print(logical_edits_comparison)
```

<https://bradleyboehmke.github.io/HOML/process.html> Sensitivity:
$\frac{TP}{TP + FN}$ Specificity: $\frac{TN}{TN + FP}$ Precision /
Positive-predictive value: $\frac{TP}{TP + FP}$ Accuracy:
$\frac{TP + TN}{total}$

```{r General Model performance}
library(pROC)
library(xtable)


## Model comparison table creation
# ML statistics
accuracy <- c(accuracy_logical, logistic_matrix$overall[['Accuracy']], knn_matrix$overall[['Accuracy']], rf_matrix$overall[['Accuracy']])

sensitivity <- c(sensitivity_logical, logistic_matrix$byClass[['Sensitivity']], knn_matrix$byClass[['Sensitivity']], rf_matrix$byClass[['Sensitivity']])

specificity <- c(specificity_logical, logistic_matrix$byClass[['Specificity']], knn_matrix$byClass[['Specificity']], rf_matrix$byClass[['Specificity']])

ppv <- c(ppv_logical, logistic_matrix$byClass[['Pos Pred Value']], knn_matrix$byClass[['Pos Pred Value']],rf_matrix$byClass[['Pos Pred Value']])


ml_table <- data.frame(sensitivity, specificity, ppv, accuracy)
ml_comparison <- as.data.frame(t(ml_table))
colnames(ml_comparison) <- c("Logical edits", "Logistic", "KNN", "RF")

# ML metrics (comparison of ML models)
xtable(ml_comparison, digits=4)
print(ml_comparison)
```

# ACS

```{r Loading ACS data}
library(readr)
setwd(data_path)

ACS_variables <- c("hisp", "spanish", "race", "bpld", "empstat", "poverty", "married", "cit_spouse", "nonfluent", "bpl_asia", "asian", "black", "white", "fem", "numprec", "yrsusa1", "hinscare", "hinscaid", "undocu", "bpl_usa", "age", "yrsed")  


ACS <- read_csv("ML Training Sample.csv")
View(ACS)
```

```{r Preparing ACS data}
ACS_sipp <- ACS %>%
  mutate(
    spanish_hispanic_latino = as.factor(ifelse(hisp==1 | spanish==1, 1, 0)),
    central_latino = as.factor(ifelse((hisp==1) & (bpld=="belize/british honduras" | bpld=="costa rica" | bpld=="el salvador" | bpld=="guatemala" | bpld=="honduras" | bpld=="nicaragua" | bpld=="panama" | bpld=="mexico"), 1, 0)),
    black = as.factor(ifelse(race=="black/african america", 1, 0)),
    white = as.factor(ifelse(race=="white", 1, 0)),
    employed = as.factor(ifelse(empstat==1, 1, 0)),
    poverty = as.factor(ifelse(poverty<100, 1, 0)),
    married = as.factor(married),
    cit_spouse = as.factor(cit_spouse),
    nonfluent = as.factor(nonfluent),
    bpl_asia = as.factor(bpl_asia),
    asian = as.factor(asian),
    fem = as.factor(fem),
    household_size = numprec,
    #Change SIPP 0 years_us to NA
    years_us = yrsusa1,
    medicare = as.factor(ifelse(hinscare=="yes", 1, 0)),
    medicaid = as.factor(ifelse(hinscaid=="has insurance through medicaid", 1, 0))
    
  )
ACS_sipp$years_us <- replace(ACS_sipp$years_us, ACS_sipp$yrsusa1=="n/a or less than one year" & ACS_sipp$bpl_usa==1, NA)
ACS_sipp$years_us <- replace(ACS_sipp$years_us, ACS_sipp$yrsusa1=="n/a or less than one year" & ACS_sipp$bpl_usa==0, 0)
ACS_sipp$years_us <- as.numeric(ACS_sipp$years_us)

ACS_sipp$other_race <- as.factor(ifelse(ACS$black!=1 & ACS$white!=1 & ACS$asian!=1, 1, 0))
ACS_sipp_na <- ACS_sipp  %>%
  filter(undocu==1, !is.na(years_us),!is.na(medicaid), !is.na(age),!is.na(fem), !is.na(married),!is.na(cit_spouse), !is.na(nonfluent),  !is.na(spanish_hispanic_latino), !is.na(central_latino), !is.na(bpl_asia), !is.na(household_size), !is.na(poverty), !is.na(asian), !is.na(black), !is.na(white), !is.na(other_race), !is.na(employed),  !is.na(yrsed))

ACS_cols_numeric <- c("central_latino", "bpl_asia", "medicaid", "age", "fem", "married", "cit_spouse", "nonfluent", "spanish_hispanic_latino", "household_size", "poverty", "asian", "black", "white", "other_race", "employed", "years_us","yrsed")

ACS_sipp_knn <- ACS_sipp_na %>%
  mutate_at(ACS_cols_numeric, as.numeric)
```

\

```{r Undocu imputation}
setwd(data_path)
## Imputation method 1: Logistic regression 
ACS_sipp_na$undocu_logistic <- predict(logistic_model, ACS_sipp_na)
write.csv(ACS_sipp_na, "ACS_SIPP_logistic.csv", row.names = FALSE)

## Imputation method 2: KNN 
ACS_sipp_na$knn_undocu <- predict(knn_model, ACS_sipp_knn)
write.csv(ACS_sipp_na, "ACS_SIPP_knn.csv", row.names = FALSE)

## Imputation method 3: RF 
ACS_sipp_na$rf_undocu <- predict(rf_model, ACS_sipp_na)
write.csv(ACS_sipp_na, "ACS_SIPP_rf.csv", row.names = FALSE)

## Imputation method 4: GBM
setwd(data_path)
ACS_sipp_na <- ACS_sipp_na %>%
  mutate(
    caret_undocu_p = predict(caret_gbm_model, type = "prob", n.trees = optimal_trees_gbm, ACS_sipp_na)[, 2],
    
    caret_undocu_q = case_when(
      caret_undocu_p < quartile_thresholds_caret[2] ~ "Q1",
      caret_undocu_p > quartile_thresholds_caret[2] & caret_undocu_p < quartile_thresholds_caret[3] ~ "Q2",
      caret_undocu_p > quartile_thresholds_caret[3] & caret_undocu_p < quartile_thresholds_caret[4] ~ "Q3",
      caret_undocu_p > quartile_thresholds_caret[4] ~ "Q4",
      TRUE ~ "Unknown"
      ),
    caret_high_prob = ifelse(caret_undocu_q=="Q4", 1, 0),
    caret_low_prob = ifelse(caret_undocu_q=="Q1", 1, 0),
    caret_high_recall = ifelse(caret_undocu_p > threshold_75, 1, 0)
  )


ACS_sipp_na <- ACS_sipp_na %>%
  mutate(
    gbm_undocu_p = predict(gbm_model, type = "response", ACS_sipp_na, n.trees = optimal_trees_gbm_two),
    
    gbm_undocu_q = case_when(
      gbm_undocu_p < quartile_thresholds_gbm[2] ~ "Q1",
      gbm_undocu_p > quartile_thresholds_gbm[2] & gbm_undocu_p < quartile_thresholds_gbm[3] ~ "Q2",
      gbm_undocu_p > quartile_thresholds_gbm[3] & gbm_undocu_p < quartile_thresholds_gbm[4] ~ "Q3",
      gbm_undocu_p > quartile_thresholds_gbm[4] ~ "Q4",
      TRUE ~ "Unknown"
      ),
    gbm_high_prob = ifelse(gbm_undocu_q=="Q4", 1, 0),
    gbm_low_prob = ifelse(gbm_undocu_q=="Q1", 1, 0),
    gbm_high_recall = ifelse(gbm_undocu_p > threshold_75_num, 1, 0)
  )

write.csv(ACS_sipp_na, "ACS_SIPP_gbm.csv", row.names = FALSE)




```
