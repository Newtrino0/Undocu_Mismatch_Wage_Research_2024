---
title: "Leading ML model"
format: html
editor: visual
---

```{r}
data_path <- "G:/Shared drives/Undocu Research/Data"
figures_path <- "G:/Shared drives/Undocu Research/Output/Figures"

setwd(data_path)

library(readr)
sipp08_2 <- read_csv("(Step 1 output) Core_TM SIPP 2008 Wave 2.csv")
View(sipp08_2)
```

## Data Preparation

```{r}
library(dplyr)
sipp08_2 <- sipp08_2 %>%
  mutate(
    undocu_entry = as.factor(ifelse(timstat=="Other", 1, 0)),
    undocu_likely = as.factor(ifelse(timstat=="Other" & eadjust=="No", 1, 0)),
    education = case_when(
      eeducate == "10th Grade"  | eeducate == "11th Grade" | eeducate == "12th grade, no diploma" | eeducate == "1st, 2nd, 3rd, or 4th grade" | eeducate == "5th Or 6th Grade" | eeducate == "7th Or 8th Grade" | eeducate == "9th Grade" | eeducate == "Less Than 1st Grade"~ "No HS diploma",
      eeducate == "Diploma or certificate from a" | eeducate == "High School Graduate - (diploma" ~ "HS diploma",
      eeducate == "Some college, but no degree" ~ "Some college",
      eeducate =="Associate (2-yr) college degree" ~ "Associate's",
      eeducate == "Bachelor's degree (for example:" ~ "Bachelor's",
      eeducate == "Master's degree (For example: MA," ~ "Master's",
      eeducate == "Doctorate degree (for example:" ~ "PhD",
      TRUE ~ "Unknown" # Default case
    ),
    yrsed = case_when(
      eeducate == "10th Grade"~10,
      eeducate == "11th Grade"~11,
      eeducate == "12th grade, no diploma" | eeducate == "Diploma or certificate from a" | eeducate == "High School Graduate - (diploma" | eeducate == "Some college, but no degree" ~12,
      eeducate == "1st, 2nd, 3rd, or 4th grade"~2.5,
      eeducate == "5th Or 6th Grade"~5.5,
      eeducate == "7th Or 8th Grade"~7.5,
      eeducate == "9th Grade"~9,
      eeducate == "Less Than 1st Grade"~0,
      eeducate =="Associate (2-yr) college degree"~14,
      eeducate == "Bachelor's degree (for example:"~16,
      eeducate == "Master's degree (For example: MA,"~17.5,
      eeducate == "Doctorate degree (for example:"~22,
      eeducate == "Professional School degree (for"~16,
      TRUE ~ NA
    ),
    college = as.factor(ifelse(eeducate=="Bachelor's degree (for example:" | eeducate=="Master's degree (For example: MA," | eeducate=="Doctorate degree (for example:", 1, 0)),
    hs_only = as.factor(ifelse(eeducate=="Some college, but no degree" | eeducate== "Associate (2-yr) college degree" | eeducate=="High School Graduate - (diploma" | eeducate=="Diploma or certificate from a", 1, 0)),
    immig_yr = case_when(
      tmoveus == "1961"~1961,
      tmoveus == "1961-1968"~1966,
      tmoveus == "1969-1973"~1971,
      tmoveus == "1974-1978"~1976,
      tmoveus == "1979-1980"~1980,
      tmoveus == "1981-1983"~1982,
      tmoveus == "1984-1985"~1984,
      tmoveus == "1986-1988"~1987,
      tmoveus == "1989-1990"~1989,
      tmoveus == "1991-1992"~1991,
      tmoveus == "1993-1994"~1993,
      tmoveus == "1995-1996"~1995,
      tmoveus == "1997-1998"~1998,
      tmoveus == "1999"~1999,
      tmoveus == "2000"~2000,
      tmoveus == "2001"~2001,
      tmoveus == "2002-2003"~2002,
      tmoveus == "2004"~2004,
      tmoveus == "2005"~2005,
      tmoveus == "2006"~2006,
      tmoveus == "2007"~2007,
      tmoveus == "2008-2009"~2009,
      TRUE ~ 0 # Default case
    ),
    married = as.factor(ifelse(ems=="Married, spouse absent" | ems=="Married, spouse present", 1, 0)),
    english_difficult = as.factor(ifelse(ehowwell=="Not at all" | ehowwell=="Not well", 1, 0)),
    nonfluent = as.factor(ifelse(ehowwell=="Not at all" | ehowwell=="Not well", 1, 0)),
    english_home = as.factor(ifelse(tlang1=="Not in Universe", 1, 0)),
    spanish_hispanic_latino = as.factor(ifelse(eorigin=="Yes", 1, 0)),
    medicaid = as.factor(ifelse(rcutyp57=="Yes, covered", 1, 0)),
    household_size = ehhnumpp,
    race = case_when(
      erace=="Asian alone" ~ "Asian",
      erace=="Black alone" ~ "Black",
      erace=="White alone" ~ "White",
      erace=="Residual" ~ "Other",
      TRUE ~ "Unknown"
    ),
    fem = as.factor(ifelse(esex=="Female", 1, 0)),
    asian = as.factor(ifelse(erace=="Asian alone", 1, 0)),
    black = as.factor(ifelse(erace=="Black alone", 1, 0)),
    white = as.factor(ifelse(erace=="White alone", 1, 0)),
    other_race = as.factor(ifelse(erace=="Residual", 1, 0)),
    employed = as.factor(ifelse(rmesr=="With a job at least 1 but not all" | rmesr=="With a job entire month, absent" | rmesr=="With a job entire month, worked", 1, 0)),
    years_us = rhcalyr - immig_yr,
    citizen = as.factor(ifelse(ecitizen=="Yes", 1, 0)),
    cit_spouse = as.factor(cit_spouse),
    poverty = as.factor(ifelse(thearn<rhpov, 1, 0)),
    armed_forces = as.factor(ifelse(eafnow=="Yes" | eafever=="Yes", 1, 0)),
    health_ins= as.factor(ifelse(rcutyp57=="Yes, covered" | rcutyp58=="Yes, covered" , 1, 0)),
    medicare = as.factor(ifelse(ecrmth=="Yes, covered", 1, 0)),
    social_security = as.factor(ifelse(rcutyp01=="Yes, covered" | rcutyp03=="Yes, covered", 1, 0)),
    central_latino = as.factor(ifelse(tbrstate=="Central America" & eorigin=="Yes", 1, 0)),
    bpl_usa = as.factor(ifelse(ebornus=="Yes", 1, 0)),
    bpl_asia = as.factor(ifelse(tbrstate == "Eastern Asia"| tbrstate == "South Central Asia"| tbrstate == "South East Asia, West Asia,", 1, 0)),
    top_ten_states = as.factor(ifelse(tfipsst=="California" | tfipsst=="Texas" | tfipsst=="Florida" | tfipsst=="New Jersey" | tfipsst=="Illinois" | tfipsst=="New York" | tfipsst=="North Carolina" | tfipsst=="Georgia" | tfipsst=="Washington" | tfipsst=="Arizona", 1, 0))
  )

sipp08_2$bpl_foreign <- as.factor(ifelse(sipp08_2$bpl_usa==1, 0, 1))
sipp08_2$undocu_likely <- replace(sipp08_2$undocu_likely, sipp08_2$immig_yr <= 1961, 0)
sipp08_2$years_us <- ifelse(sipp08_2$years_us == 2008 | sipp08_2$years_us == 2009 | sipp08_2$years_us == -1 , NA, sipp08_2$years_us)
sipp08_2$tage <- replace(sipp08_2$tage, sipp08_2$tage == "Less than 1 full year old", 0)
sipp08_2$age <- as.numeric(sipp08_2$tage)
sipp08_2$undocu_likely <- replace(sipp08_2$undocu_likely, sipp08_2$armed_forces==1 | sipp08_2$social_security==1, 0 )
sipp08_2$undocu_logical <- as.factor(ifelse(sipp08_2$citizen==0 & (sipp08_2$armed_forces==0 | sipp08_2$medicare==0 | sipp08_2$social_security==0), 1, 0))
sipp08_2$id <- seq_len(nrow(sipp08_2))



# Define column sets for different analyses
variable_lists <- list(
  base = c("undocu_likely", "central_latino", "bpl_asia", "medicaid", "age", "fem", 
           "married", "cit_spouse", "nonfluent", "spanish_hispanic_latino", 
           "household_size", "poverty", "asian", "black", "white", "other_race", 
           "employed", "years_us", "yrsed"),
  
  descriptive = c("undocu_likely", "undocu_logical", "bpl_foreign", "medicaid", 
                  "central_latino", "bpl_asia", "age", "fem", "married", "cit_spouse", 
                  "nonfluent", "spanish_hispanic_latino", "household_size", "poverty", 
                  "asian", "black", "white", "other_race", "employed", "years_us", "yrsed")

)

# Create analysis datasets
create_datasets <- function(data, variables) {
  datasets <- list()
  
  # Filter for undocu_logical == 1
  undocu_filter <- data[data$undocu_logical == 1, ]
  
  datasets$dTable <- as.data.frame(undocu_filter[, variables$descriptive]) %>%
    mutate_at(vars(-undocu_likely), as.numeric) %>%
    na.omit()
  
  datasets$sample <- as.data.frame(undocu_filter[, variables$base]) %>%
    na.omit()
  
  datasets$knn <- as.data.frame(undocu_filter[, variables$base]) %>%
    mutate_at(vars(-undocu_likely), as.numeric) %>%
    na.omit()
  
  
  # Demographic subsets (college graduates only)
  datasets$noncit <- data[data$citizen == 0 & data$college == 1, ]
  datasets$central_latino <- data[data$central_latino == 1 & data$college == 1, ]
  datasets$spanish_hispanic_latino <- data[data$spanish_hispanic_latino == 1 & data$college == 1, ]
  datasets$top_states <- data[data$top_ten_states == 1 & data$college == 1, ]
  
  return(datasets)
}


# Create all analysis datasets
all_data <- create_datasets(sipp08_2, variable_lists)



View(sipp08_2)

setwd("G:/Shared drives/Undocu Research/Data")
write.csv(sipp08_2, "SIPP08_2.csv", row.names = FALSE)
```

## GBM Library 

```{r}
library(gbm)      # For Gradient Boosting
library(class)
library(caTools)
library(caret)
library(ROCR)
library(vip)  # For feature importance
library(gridExtra)

control_up <- trainControl(
    method = "cv",
    number = 10,  
    summaryFunction = twoClassSummary,
    classProbs = TRUE,
    sampling = "up",
)


## Data partitioning for gbm library
all_data$sample_num <- all_data$sample
all_data$sample_num$undocu_likely_num <- as.numeric(all_data$sample_num$undocu_likely) - 1
train_index_gbm_num <- createDataPartition(all_data$sample_num$undocu_likely_num, p = 0.7, list = FALSE)
train_gbm_num <- all_data$sample_num[train_index_gbm_num, ]
test_gbm_num <- all_data$sample_num[-train_index_gbm_num, ]
                   



gbm_model <- gbm(undocu_likely_num ~ age + fem + married + cit_spouse + medicaid + nonfluent + spanish_hispanic_latino + central_latino + bpl_asia + household_size + poverty + asian + black + white + other_race + employed + years_us + yrsed, data = train_gbm_num, 
                 verbose = FALSE,
                 cv.folds = 10,
                 n.trees = 1766,
                 shrinkage = 0.005, #Optimimal
                 interaction.depth = 6)

## Model summary and optimal number of trees
gbm_model
print(gbm_model$bestTune)
optimal_trees_gbm_two <- gbm_model$bestTune$n.trees


setwd(figures_path)
## Feature importance
feature_importance_gbm <- vip(gbm_model, num_features = 19, bar = FALSE)
ggsave("gbm_feature_importance.png", feature_importance_gbm, width = 22, height = 16, units = "cm", dpi = 300)
grid.arrange(feature_importance_gbm, nrow = 1)

```

## Caret Library

```{r}
## Data partitioning for caret library
levels(all_data$sample$undocu_likely) <- make.names(levels(all_data$sample$undocu_likely))
train_index_gbm <- createDataPartition(all_data$sample$undocu_likely, p = 0.7, list = FALSE)
train_gbm <- all_data$sample[train_index_gbm, ]
test_gbm <- all_data$sample[-train_index_gbm, ]

caret_gbm_model <- train(
  undocu_likely ~ age + fem + married + cit_spouse + medicaid + nonfluent + spanish_hispanic_latino + central_latino + bpl_asia + household_size + poverty + asian + black + white + other_race + employed + years_us + yrsed,
  data = train_gbm,
  method = "gbm",
  trControl = control_up,
  metric = "ROC",                       
  verbose = FALSE
)

caret_gbm_model
print(caret_gbm_model$bestTune)
optimal_trees_gbm <- caret_gbm_model$bestTune$n.trees

setwd(figures_path)
## Feature importance
feature_importance_caret <- vip(caret_gbm_model, num_features = 19, bar = FALSE)
ggsave("caret_feature_importance.png", feature_importance_caret, width = 22, height = 16, units = "cm", dpi = 300)
grid.arrange(feature_importance_caret, nrow = 1)
```

## GBM Performance

```{r}
# ==============================================================================
# Caret Library Performance
# ==============================================================================
# Make predictions on test set
boost.pred = predict(caret_gbm_model, newdata = test_gbm, type = "prob", n.trees = optimal_trees_gbm)[, 2]  # Get probabilities for class "1"
# Create prediction object for ROCR
boost.prediction = prediction(boost.pred, test_gbm$undocu_likely)
# Get raw predictions (class labels)
boost.predict = predict(caret_gbm_model, newdata = test_gbm, n.trees=optimal_trees_gbm)  # Class predictions

boost.roc = performance(boost.prediction,"tpr","fpr")   # ROC curve 
boost.pr = performance(boost.prediction,"prec","rec") # Precision-Recall curve
boost.auc <- performance(boost.prediction,"auc")@y.values[[1]] # AUC value

boost_auc_pr <- trapz(
  as.vector(boost.pr@x.values)[[1]][-1], 
  as.vector(boost.pr@y.values)[[1]][-1]
)

gbm_performance <- boost.pr
plot(boost.roc, main = "ROC Curve", col = "blue", lwd = 2) #PLOT EACH ON A SINGLE GRAPH FOR ROC
abline(a = 0, b = 1, lty = 2, col = "red")
plot(boost.pr) # plot Precision-Recall curve


cat("Optimal number of trees:", optimal_trees_gbm, "\n")
cat("AUC-ROC:", round(boost.auc, 4), "\n")
cat("AUC-PR:", round(boost_auc_pr, 4), "\n")

caret_predicted_class <- ifelse(boost.pred > 0.5, 1, 0)
table(Predicted = predicted_class, Actual = test_gbm$undocu_likely)
plot(caret_gbm_model)


# ==============================================================================
# GBM Library Performance
# ==============================================================================

## Metrics for Figure 2 comparison (gbm library model)
# Make predictions on test set
gbm.pred = predict(gbm_model, newdata = test_gbm_num, type = "response", n.trees = optimal_trees_gbm_two)
# Create prediction object for ROCR
gbm.prediction = prediction(gbm.preds, test_gbm_num$undocu_likely)
# Get raw predictions (class labels)
gbm.predict = predict(gbm_model, newdata = test_gbm_num, n.trees=optimal_trees_gbm_two)  # Class predictions

gbm.roc = performance(gbm.prediction,"tpr","fpr")   # ROC curve 
gbm.pr = performance(gbm.prediction,"prec","rec") # Precision-Recall curve
gbm.auc <- performance(gbm.prediction,"auc")@y.values[[1]] # AUC value

gbm_auc_pr <- trapz(
  as.vector(gbm.pr@x.values)[[1]][-1], 
  as.vector(gbm.pr@y.values)[[1]][-1]
)

gbm_performance <- gbm.pr
plot(gbm.roc, main = "ROC Curve", col = "blue", lwd = 2) #PLOT EACH ON A SINGLE GRAPH FOR ROC
abline(a = 0, b = 1, lty = 2, col = "red")
plot(gbm.pr) # plot Precision-Recall curve


cat("Optimal number of trees:", optimal_trees_gbm_two, "\n")
cat("AUC-ROC:", round(gbm.auc, 4), "\n")
cat("AUC-PR:", round(gbm_auc_pr, 4), "\n")

gbm_predicted_class <- ifelse(gbm.pred > 0.5, 1, 0)
table(Predicted = gbm_predicted_class, Actual = test_gbm_num$undocu_likely)
#plot(gbm_model) # Not same as plot for caret
```

## P Quartiles and Thresholds

```{r}
test_gbm$p = predict(caret_gbm_model, newdata = test_gbm, type = "prob", n.trees = optimal_trees_gbm)[, 2]
test_gbm_num$p = predict(gbm_model, newdata = test_gbm_num, type = "response", n.trees = optimal_trees_gbm_two)


# Create quartiles based on probability magnitude
test_gbm$p_quartiles <- cut(test_gbm$p, 
                                   breaks = quantile(test_gbm$p, probs = c(0, 0.25, 0.5, 0.75, 1)), 
                                   labels = c("Q1", "Q2", "Q3", "Q4"), 
                                   include.lowest = TRUE)

test_gbm_num$p_quartiles <- cut(test_gbm_num$p, 
                                   breaks = quantile(test_gbm_num$p, probs = c(0, 0.25, 0.5, 0.75, 1)), 
                                   labels = c("Q1", "Q2", "Q3", "Q4"), 
                                   include.lowest = TRUE)
# Check the distributions
table(test_gbm$p_quartiles)
table(test_gbm_num$p_quartiles)

# Proportion of undocu_likely in each quartiles
quartiles_table <- test_gbm %>%
  group_by(p_quartiles) %>%
  summarise(
    count = sum(undocu_likely == "X1"),
    n = n(),
    actual_proportion = (count / n)
    )
quartiles_table_gbm <- test_gbm_num %>%
  group_by(p_quartiles) %>%
  summarise(
    count = sum(undocu_likely == 1),
    n = n(),
    actual_proportion = (count / n)
    )

xtable(quartiles_table, digits=4)
print(quartiles_table)
xtable(quartiles_table_gbm, digits=4)
print(quartiles_table_gbm)


top_75_sample <- test_gbm %>%
  filter(undocu_likely == "X1") %>%
  arrange(desc(p)) %>%
  mutate(
    rank = row_number(),
    cumulative_p = rank / n(),
    top_75 = cumulative_p <= 0.75
  ) %>%
  filter(top_75==TRUE)
threshold_75 <- min(top_75_sample$p)

top_75_sample_num <- test_gbm_num %>%
  filter(undocu_likely == 1) %>%
  arrange(desc(p)) %>%
  mutate(
    rank = row_number(),
    cumulative_p = rank / n(),
    top_75 = cumulative_p <= 0.75
  ) %>%
  filter(top_75==TRUE)
threshold_75_num <- min(top_75_sample_num$p)

# Create GBM high recall (p>threshold), high probability (75% of actual), low probability (Q1)
test_gbm$prediction <- ifelse(test_gbm$p >= threshold_75, 1, 0)
test_gbm_num$predicted_class <- ifelse(test_gbm_num$p >= threshold_75_num, 1, 0)

# ==============================================================================
# PREPARE PRECISION-RECALL DATA FOR ALL MODELS
# ==============================================================================

```

## Precision-Recall Curves

```{r}
setwd(figures_path)
# ==============================================================================
# PREPARE PRECISION-RECALL DATA FOR ALL MODELS
# ==============================================================================

# Start with boosted trees as baseline
boost_pr_data <- data.frame(
  recall = unlist(boost.pr@x.values), 
  precision = unlist(boost.pr@y.values)
)

# Interpolate other models to match boosted trees recall values
gbm_pr_data <- data.frame(
  recall = approx(unlist(gbm.pr@x.values), unlist(gbm.pr@y.values), 
                 xout = boost_pr_data$recall)$x,
  precision = approx(unlist(gbm.pr@x.values), unlist(gbm.pr@y.values), 
                    xout = boost_pr_data$recall)$y
)



# ==============================================================================
# COMBINE ALL DATA
# ==============================================================================

all_pr_data <- boost_pr_data

# Add precision values for other models
all_pr_data$gbm_precision <- gbm_pr_data$precision

# Calculate precision differences relative to boosted trees
all_pr_data$gbm_precision_difference <- gbm_pr_data$precision - all_pr_data$precision

# ==============================================================================
# PLOT 1: PRECISION-RECALL CURVES FOR ALL MODELS
# ==============================================================================

#pdf("undocumented_pr_curves.pdf", family="Times", width=9)
png("undocumented_pr_curves.png", width = 22*100, height = 16*100, res = 300, family = "Times")

op <- par(family = "serif")

plot(all_pr_data$recall, all_pr_data$precision, type = "l", pch = 19,
     col = "black", xlab = "Recall", ylab = "Precision",
     lwd = 3, ylim=c(0, 1), xlim = c(0,1), lty = 1, cex.lab=1.5, cex.axis=1)

lines(all_pr_data$recall, all_pr_data$gbm_precision, type = "l", 
      col = "yellow", lwd = 3, lty = 2)
lines(all_pr_data$recall, all_pr_data$rf_precision, type = "l", 
      col = "blue", lwd = 3, lty = 3)
lines(all_pr_data$recall, all_pr_data$logistic_precision, type = "l",
      col = "red", lwd = 3, lty = 4)
lines(all_pr_data$recall, all_pr_data$knn_precision, type = "l",
      col = "green", lwd = 3, lty = 5)

legend(x = 0.5, y = 1, 
       col = c("white", "black", "yellow", "blue", "red", "green"),
       lwd = c(3, 3, 3, 3, 3, 3), 
       lty = c(0, 1, 2, 3, 4, 5),
       legend = c("Model:", "Boosted Trees", "GBM Library", "Random Forest", "Logistic", "KNN"),
       bty = 'n', cex = 1.25, ncol = 1, seg.len = 6)

# Add random baseline
random_baseline <- total_undocu / nrow(test_gbm)
abline(h = random_baseline, lty = 2, col = "gray")

dev.off()
```

## Demographic Precision-Recall Curve (Caret library, Figure 3)

```{r}
# Total undocumented cases in test set
total_undocu <- sum(test_gbm$undocu_likely == "X1")

# Group 1: Top 10 states
recall_top_ten <- sum(test_gbm$undocu_likely == "X1" & test_gbm$top_ten_states == 1) / total_undocu
precis_top_ten <- sum(test_gbm$undocu_likely == "X1" & test_gbm$top_ten_states == 1) / 
                sum(test_gbm$top_ten_states == 1)

# Group 2: Hispanic/Latino
recall_hispanic <- sum(test_gbm$undocu_likely == "X1" & test_gbm$spanish_hispanic_latino == 1) / total_undocu
precis_hispanic <- sum(test_gbm$undocu_likely == "X1" & test_gbm$spanish_hispanic_latino == 1) / 
                   sum(test_gbm$spanish_hispanic_latino == 1)

# Group 3: Central American Latino
recall_central <- sum(test_gbm$undocu_likely == "X1" & 
                           test_gbm$central_latino ==   1) / total_undocu
precis_central <- sum(test_gbm$undocu_likely == "X1" & 
                           test_gbm$central_latino == 1) / 
                        sum(test_gbm$central_latino == 1)

# Group 4: Asian-born
recall_asia <- sum(test_gbm$undocu_likely == "X1" & test_gbm$bpl_asia == 1) / total_undocu
precis_asia <- sum(test_gbm$undocu_likely == "X1" & test_gbm$bpl_asia == 1) / 
               sum(test_gbm$bpl_asia == 1)

# Group 5: Non-fluent English speakers
recall_nonfluent <- sum(test_gbm$undocu_likely == "X1" & test_gbm$nonfluent == 1) / total_undocu
precis_nonfluent <- sum(test_gbm$undocu_likely == "X1" & test_gbm$nonfluent == 1) / 
                    sum(test_gbm$nonfluent == 1)


# Set output file
setwd(figures_path)
png("undocumented_demographic_pr_curve.png", width = 2700, height = 2100, res = 300, family = "Times")
# Or use: pdf("undocumented_demographic_pr_curve.pdf", family="Times", width=9)

# Set up plot parameters
op <- par(family = "serif")

# Main precision-recall curve
plot(boost_pr_data$recall, boost_pr_data$precision, type = "l", pch = 19,
     col = "black", xlab = "Recall", ylab = "Precision",
     lwd = 3,
     ylim = c(0, 1), xlim = c(0, 1), lty = 1, 
     cex.lab = 1.5, cex.axis = 1,
     main = "")  # Add title if desired

# Add demographic group points
points(recall_top_ten, precis_top_ten, pch = 17, col = 1, cex = 1.5)
points(recall_hispanic, precis_hispanic, pch = 19, col = 1, cex = 1.5)
points(recall_central, precis_central, pch = 19, col = "gray", cex = 1.5)
points(recall_asia, precis_asia, pch = 17, col = "gray", cex = 1.5)
points(recall_nonfluent, precis_nonfluent, pch = 15, col = "darkgray", cex = 1.5)


# Demographic groups legend
legend(0.25, 1, 
       col = c("white", 1, 1, "gray", "gray", "darkgray"), 
       pch = c(19, 17, 19, 19, 17, 15),
       pt.cex = 1.5,
       legend = c("Demographic Groups:", 
                 "Top 10 states", 
                 "Hispanic/Latino",
                 "Central Latino", 
                 "Asian Born",
                 "Non-fluent English"),
       bty = 'n', 
       cex = 1.25, 
       ncol = 1, 
       bg = "white")

# Add random baseline
random_baseline <- sum(test_gbm$undocu_likely == 1) / nrow(test_gbm)
abline(h = random_baseline, lty = 2, col = "gray50")

# Add text label for random baseline
text(0.5, random_baseline + 0.05, 
     paste("Random Baseline:", round(random_baseline, 3)), 
     cex = 0.9, col = "gray30")

dev.off()



cat("\n=== DEMOGRAPHIC GROUP PERFORMANCE ===\n")
cat("Random Baseline Precision:", round(random_baseline, 4), "\n\n")

summary_df <- data.frame(
  Group = c("Top 10 states", "Hispanic/Latino", "Central Latino", 
            "Asian Born", "Non-fluent English"),
  Recall = c(recall_top_ten, recall_hispanic, recall_central, 
            recall_asia, recall_nonfluent),
  Precision = c(precis_top_ten, precis_hispanic, precis_central, 
               precis_asia, precis_nonfluent)
)

print(summary_df)

# Save as both PDF and PNG
create_demographic_plot <- function(filename_base) {
  # Function to create the plot (reusable)
  make_plot <- function() {
    par(family = "serif")
    plot(boost_pr_data$recall, boost_pr_data$precision, type = "l", pch = 19,
         col = "black", xlab = "Recall", ylab = "Precision",
         lwd = 3, ylim = c(0, 1), xlim = c(0, 1), lty = 1, 
         cex.lab = 1.5, cex.axis = 1)
    
    points(recall_top_ten, precis_top_ten, pch = 17, col = 1, cex = 1.5)
    points(recall_hispanic, precis_hispanic, pch = 19, col = 1, cex = 1.5)
    points(recall_central, precis_central, pch = 19, col = "gray", cex = 1.5)
    points(recall_asia, precis_asia, pch = 17, col = "gray", cex = 1.5)
    
    legend(0.63, 1, col = c("white", 1), lwd = 2, lty = c(0, 1),
           legend = c("Learning Method:", "Boosted Trees"),
           bty = 'n', cex = 1.25, ncol = 1)
    
    legend(0.30, 1, col = c("white", 1, 1, "gray", "gray"), 
           pch = c(19, 17, 19, 19, 17), pt.cex = 1.5,
           legend = c("Groups:", "Top 10 states", "Hispanic/Latino",
                     "Central Latino", "Asian Born"),
           bty = 'n', cex = 1.25, ncol = 1, bg = "white")
    
    abline(h = random_baseline, lty = 2, col = "gray50")
  }
  
  # Save as PDF
  pdf(paste0(filename_base, ".pdf"), family = "Times", width = 9)
  make_plot()
  dev.off()
  
  # Save as PNG
  png(paste0(filename_base, ".png"), width = 2700, height = 2100, res = 300, family = "Times")
  make_plot()
  dev.off()
}
```

## Demographic Precision-Recall Curve (GBM Library; Figure 3)

```{r}
# Total undocumented cases in test set
total_undocu_gbm <- sum(test_gbm_num$undocu_likely == 1)

# Group 1: Top 10 states
recall_top_ten_gbm <- sum(test_gbm_num$undocu_likely == 1 & test_gbm_num$top_ten_states == 1) / total_undocu_gbm
precis_top_ten_gbm <- sum(test_gbm_num$undocu_likely == 1 & test_gbm_num$top_ten_states == 1) / 
                sum(test_gbm_num$top_ten_states == 1)

# Group 2: Hispanic/Latino
recall_hispanic_gbm <- sum(test_gbm_num$undocu_likely == 1 & test_gbm_num$spanish_hispanic_latino == 1) / total_undocu_gbm
precis_hispanic_gbm <- sum(test_gbm_num$undocu_likely == 1 & test_gbm$spanish_hispanic_latino == 1) / 
                   sum(test_gbm_num$spanish_hispanic_latino == 1)

# Group 3: Central American Latino
recall_central_gbm <- sum(test_gbm_num$undocu_likely == 1 & 
                           test_gbm_num$central_latino ==   1) / total_undocu_gbm
precis_central_gbm <- sum(test_gbm_num$undocu_likely == 1 & 
                           test_gbm_num$central_latino == 1) / 
                        sum(test_gbm_num$central_latino == 1)

# Group 4: Asian-born
recall_asia_gbm <- sum(test_gbm_num$undocu_likely == 1 & test_gbm_num$bpl_asia == 1) / total_undocu_gbm
precis_asia_gbm <- sum(test_gbm_num$undocu_likely == 1 & test_gbm_num$bpl_asia == 1) / 
               sum(test_gbm_num$bpl_asia == 1)

# Group 5: Non-fluent English speakers
recall_nonfluent_gbm <- sum(test_gbm_num$undocu_likely == 1 & test_gbm_num$nonfluent == 1) / total_undocu_gbm
precis_nonfluent_gbm <- sum(test_gbm_num$undocu_likely == 1 & test_gbm_num$nonfluent == 1) / 
                    sum(test_gbm_num$nonfluent == 1)


# Set output file
setwd(figures_path)
png("undocumented_demographic__gbm_pr_curve.png", width = 2700, height = 2100, res = 300, family = "Times")
# Or use: pdf("undocumented_demographic_pr_curve.pdf", family="Times", width=9)

# Set up plot parameters
op <- par(family = "serif")

# Main precision-recall curve
plot(gbm_pr_data$recall, gbm_pr_data$precision, type = "l", pch = 19,
     col = "black", xlab = "Recall", ylab = "Precision",
     lwd = 3,
     ylim = c(0, 1), xlim = c(0, 1), lty = 1, 
     cex.lab = 1.5, cex.axis = 1,
     main = "")  # Add title if desired

# Add demographic group points
points(recall_top_ten_gbm, precis_top_ten_gbm, pch = 17, col = 1, cex = 1.5)
points(recall_hispanic_gbm, precis_hispanic_gbm, pch = 19, col = 1, cex = 1.5)
points(recall_central_gbm, precis_central_gbm, pch = 19, col = "gray", cex = 1.5)
points(recall_asia_gbm, precis_asia_gbm, pch = 17, col = "gray", cex = 1.5)
points(recall_nonfluent_gbm, precis_nonfluent_gbm, pch = 15, col = "darkgray", cex = 1.5)


# Demographic groups legend
legend(0.25, 1, 
       col = c("white", 1, 1, "gray", "gray", "darkgray"), 
       pch = c(19, 17, 19, 19, 17, 15),
       pt.cex = 1.5,
       legend = c("Demographic Groups:", 
                 "Top 10 states", 
                 "Hispanic/Latino",
                 "Central Latino", 
                 "Asian Born",
                 "Non-fluent English"),
       bty = 'n', 
       cex = 1.25, 
       ncol = 1, 
       bg = "white")

# Add random baseline
random_baseline_gbm <- sum(test_gbm_num$undocu_likely == 1) / nrow(test_gbm_num)
abline(h = random_baseline_gbm, lty = 2, col = "gray50")

# Add text label for random baseline
text(0.5, random_baseline_gbm + 0.05, 
     paste("Random Baseline:", round(random_baseline_gbm, 3)), 
     cex = 0.9, col = "gray30")

dev.off()



cat("\n=== DEMOGRAPHIC GROUP PERFORMANCE ===\n")
cat("Random Baseline Precision:", round(random_baseline_gbm, 4), "\n\n")

summary_df_gbm <- data.frame(
  Group = c("Top 10 states", "Hispanic/Latino", "Central Latino", 
            "Asian Born", "Non-fluent English"),
  Recall = c(recall_top_ten_gbm, recall_hispanic_gbm, recall_central_gbm, 
            recall_asia_gbm, recall_nonfluent_gbm),
  Precision = c(precis_top_ten_gbm, precis_hispanic_gbm, precis_central_gbm, 
               precis_asia_gbm, precis_nonfluent_gbm)
)

print(summary_df_gbm)

# Save as both PDF and PNG
create_demographic_plot_gbm <- function(filename_base) {
  # Function to create the plot (reusable)
  make_plot <- function() {
    par(family = "serif")
    plot(gbm_pr_data$recall, gbm_pr_data$precision, type = "l", pch = 19,
         col = "black", xlab = "Recall", ylab = "Precision",
         lwd = 3, ylim = c(0, 1), xlim = c(0, 1), lty = 1, 
         cex.lab = 1.5, cex.axis = 1)
    
    points(recall_top_ten, precis_top_ten, pch = 17, col = 1, cex = 1.5)
    points(recall_hispanic, precis_hispanic, pch = 19, col = 1, cex = 1.5)
    points(recall_central, precis_central, pch = 19, col = "gray", cex = 1.5)
    points(recall_asia, precis_asia, pch = 17, col = "gray", cex = 1.5)
    
    legend(0.63, 1, col = c("white", 1), lwd = 2, lty = c(0, 1),
           legend = c("Learning Method:", "Boosted Trees"),
           bty = 'n', cex = 1.25, ncol = 1)
    
    legend(0.30, 1, col = c("white", 1, 1, "gray", "gray"), 
           pch = c(19, 17, 19, 19, 17), pt.cex = 1.5,
           legend = c("Groups:", "Top 10 states", "Hispanic/Latino",
                     "Central Latino", "Asian Born"),
           bty = 'n', cex = 1.25, ncol = 1, bg = "white")
    
    abline(h = random_baseline_gbm, lty = 2, col = "gray50")
  }
  
  # Save as PDF
  pdf(paste0(filename_base, ".pdf"), family = "Times", width = 9)
  make_plot()
  dev.off()
  
  # Save as PNG
  png(paste0(filename_base, ".png"), width = 2700, height = 2100, res = 300, family = "Times")
  make_plot()
  dev.off()
}
```

## Exporting to ACS

```{r}

```
